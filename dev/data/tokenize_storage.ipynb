{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载数据",
   "id": "dc1912676497aa1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:12:11.896748Z",
     "start_time": "2024-12-06T11:12:11.880556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from utils.file_utils import load_base_config\n",
    "\n",
    "# config = load_base_config(\"conf/config.yaml\")\n",
    "data_file = \"tang/tang.txt\""
   ],
   "id": "746c4b7e631fcf24",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:12:13.130599Z",
     "start_time": "2024-12-06T11:12:13.061591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取数据\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 去除空行和多余的空白符\n",
    "chinese_data = [line.strip() for line in lines if line.strip()]"
   ],
   "id": "8909afbfd5f2b2fe",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:12:14.409653Z",
     "start_time": "2024-12-06T11:12:14.402016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(chinese_data[:4])\n",
    "print(len(chinese_data))"
   ],
   "id": "c45316a2a37d44bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['度门能不访，冒雪屡西东。已想人如玉，遥怜马似骢。乍迷金谷路，稍变上阳宫。还比相思意，纷纷正满空。', '逍遥东城隅，双树寒葱蒨。广庭流华月，高阁凝余霰。杜门非养素，抱疾阻良䜩。孰谓无他人，思君岁云变。官曹亮先忝，陈躅慙俊彥。岂知晨与夜，相代不相见。缄书问所如，詶藻当芬绚。', '川上风雨来，须臾满城阙。岧峣青莲界，萧条孤兴发。前山遽已净，阴霭夜来歇。乔木生夏凉，流云吐华月。严城自有限，一水非难越。相望曙河远，高斋坐超忽。', '庭树忽已暗，故人那不来。祗因厌烦暑，永日坐霜台。']\n",
      "57580\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 文本切分，并用bert-base-chinese分词&存储",
   "id": "77266d4ece4d2591"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:12:20.355583Z",
     "start_time": "2024-12-06T11:12:16.404749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\", cache_dir=\"../model/bert-base-chinese\")\n",
    "tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '<|endoftext|>'})"
   ],
   "id": "794d78bee94c8037",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:12:33.244424Z",
     "start_time": "2024-12-06T11:12:21.791981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 添加结束标记 <|endoftext|> 到分词器\n",
    "tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '<|endoftext|>'})\n",
    "\n",
    "# 设置每个片段的最大 token 数\n",
    "context_length = 128\n",
    "\n",
    "def tokenize(doc):\n",
    "    # 为每个文档添加 <|endoftext|> 标记\n",
    "    text_with_eot = doc + tokenizer.eos_token  # 这里会使用已经添加的结束标记\n",
    "    tokens = tokenizer(\n",
    "        text_with_eot,\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_tensors=\"np\"\n",
    "    )[\"input_ids\"][0][1:-1]\n",
    "\n",
    "    tokens_np = np.array(tokens.flatten(), dtype=np.uint16)\n",
    "\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token 字典对于 uint16 来说太大\"\n",
    "    return tokens_np\n",
    "\n",
    "def write_datafile(filename, tokens_np):\n",
    "    np.save(filename, tokens_np)\n",
    "\n",
    "shard_size = int(1e6)  # 每个分片的 token 数\n",
    "\n",
    "output_dir = \"tang_tokenized_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 假设中文数据是一个列表，每个元素是一个文档文本\n",
    "chinese_data = chinese_data\n",
    "# chinese_data = [\"文档1的内容\", \"文档2的内容\", \"文档3的内容\"]  # 示例数据\n",
    "\n",
    "nprocs = max(1, os.cpu_count() // 2)\n",
    "# 标记所有文档并写入输出分片，每个分片shard_size令牌（最后一个分片有剩余）\n",
    "# with mp.Pool(nprocs) as pool: # 多线程\n",
    "shard_index = 0\n",
    "\n",
    "# preallocate buffer 以保存当前分片\n",
    "all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
    "token_count = 0\n",
    "progress_bar = None\n",
    "\n",
    "# 判断当前分片中是否有足够的空间用于新token？\n",
    "# for tokens in pool.imap(tokenize, chinese_data, chunksize=16):\n",
    "for token in chinese_data:\n",
    "    tokens = tokenize(token)\n",
    "    if token_count + len(tokens) < shard_size:\n",
    "        # 只需将 Token 附加到当前分片\n",
    "        all_tokens_np[token_count:token_count + len(tokens)] = tokens\n",
    "        token_count += len(tokens)\n",
    "\n",
    "        # 更新进度条\n",
    "        if progress_bar is None:\n",
    "            progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "        progress_bar.update(len(tokens))\n",
    "    else:\n",
    "        # 写入当前分片并启动新分片\n",
    "        split = \"val\" if shard_index == 0 else \"train\"\n",
    "        filename = os.path.join(output_dir, f\"tang_{split}_{shard_index:06d}.npy\")\n",
    "\n",
    "\n",
    "        # 将文档拆分为适合此分片的任何内容，其余的转到下一个\n",
    "        remainder = shard_size - token_count\n",
    "        progress_bar.update(remainder)\n",
    "        all_tokens_np[token_count:token_count + remainder] = tokens[:remainder]\n",
    "        write_datafile(filename, all_tokens_np)\n",
    "        shard_index += 1\n",
    "        progress_bar = None\n",
    "\n",
    "        # 使用当前文档的剩余部分填充下一个分片\n",
    "        all_tokens_np[0:len(tokens) - remainder] = tokens[remainder:]\n",
    "        token_count = len(tokens) - remainder\n",
    "\n",
    "# 将任何剩余的 Token 写入最后一个分片\n",
    "if token_count != 0:\n",
    "    split = \"val\" if shard_index == 0 else \"train\"\n",
    "    filename = os.path.join(output_dir, f\"tang_{split}_{shard_index:06d}.npy\")\n",
    "    write_datafile(filename, all_tokens_np[:token_count])"
   ],
   "id": "4ab79590555b962a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shard 0: 100%|██████████| 1000000/1000000 [00:03<00:00, 269260.90tokens/s]\n",
      "Shard 1: 100%|█████████▉| 999950/1000000 [00:03<00:00, 272138.89tokens/s]\n",
      "Shard 2: 100%|█████████▉| 999895/1000000 [00:03<00:00, 265373.77tokens/s]\n",
      "Shard 3:   6%|▋         | 64036/1000000 [00:00<00:02, 316518.15tokens/s]"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:14:35.850031Z",
     "start_time": "2024-12-06T11:14:35.838216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 观察npy文件\n",
    "import numpy as np\n",
    "tokens = np.load(\"tang_tokenized_data/tang_train_000003.npy\") # 1000000\n",
    "print(tokens[:300])"
   ],
   "id": "8f1da5224f26f03f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6858   511   952  3198  1249  4941   100  8024  1140  1892   727  1093\n",
      "  1216   511   100   774   782  3187  2680  8024  4506  2416  2259   679\n",
      "  4958   511   862  7557  2569   826  5572  8024  1283  6770  6590  2216\n",
      "  7599   511 21128  7075  5644  4912  1765   719  8024  5417  1310  3821\n",
      "  7345  4958   511  2516  1759  6006  1131  1743  8024  1074  2335   718\n",
      "  2533   704   511  7987  7582  6233  4635  3189  8024  7911  1355   814\n",
      "  3926  7599   511  3307  2401  6411  6874  6919  8024  2577  3341  2692\n",
      "   679  4956   511  3212  1728  2196  3805  2277  8024   791   842  6692\n",
      "  5335  2319   511  1921  1765  2552  3187  2460  8024  4868  4856  4415\n",
      "   771  1398   511  5428  1290  5425  3948  1266  8024  4373  3466   952\n",
      "  1068   691   511   830  2703  1071  7410  7349  8024  3209  1409  3193\n",
      "  1239  1216   511 21128  2295  3353   855   862  2300  8024  3175  4761\n",
      "  6863  1265  1216   511  7360  4130  6864  1760   712  8024  3307  2401\n",
      "  6134  5335  2319   511  7391  3216  6825  7471  1880  8024  2321  2288\n",
      "  1403  4819  4958   511  6496  6756  1728  1383  4448  8024  7987  7730\n",
      "  2703  1285   704   511   674  2259  1898  7270  1762  8024  1283  2334\n",
      "  3698  6760  7413   511   691  6963  3625  4670   752  8024  6205  5010\n",
      "   842  4640  7599   511 21128  7739  2273  2970  3173   705  8024   100\n",
      "   100  7730  4819  4958   511  1142  2255  2458  4908  3671  8024  7391\n",
      "  7443  5929   803  2151   511  5316  7336  4310  3411  1128  8024  7425\n",
      "  3448  2213  2372  6004   511  3946  3787  3295  3861  3189  8024  1290\n",
      "  7667  3191  6816  7599   511  5484  4946  4750   756  6776  8024  3918\n",
      "  3756  7308  5331   100   511   691  6946  3307  2401  1905  8024  4448\n",
      "  3698  7461  4088  4088   511 21128  3236  5682   707  1352  7336  8024\n",
      "  2544  5628  4851   855  7373   511  6823  2661  7987  1128   100  8024]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 观察npy文件\n",
    "import numpy as np\n",
    "tokens = np.load(\"tang_tokenized_data/tang_shard_000003.npy\") # 1000000\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ],
   "id": "b48d46179bc89a09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  测试bert-base-chinese分词器\n",
    "<br>\n",
    "\n",
    "1. 因为GPT是自回归的语言模型，不会停止输出，需要添加终止标志"
   ],
   "id": "979c548bd4040ef5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:13:50.040801Z",
     "start_time": "2024-12-06T11:13:50.030160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data = \"侯枉高鉴，举善掩瑕疵。斯民本已安，工拙两无施。何以酬明德，岁晏不磷缁。时节乃来集，欣怀方载驰。平明大府开，一得拜光辉。温如春风至，肃若严霜威。羣属所载瞻，而忘倦与饥。公堂燕华筵，礼罢复言辞。将从平门道，憩车沣水湄。山川降嘉。\"\n",
    "data = \"补吏多下迁，罢归聊自度。园庐既芜没，烟景空澹泊。闲居养疴瘵，守素甘葵藿。颜鬓日衰耗，冠带亦寥落。青苔已生路，绿筠始分箨。夕气下遥阴，微风动疎薄。草玄良见诮，杜门无请讬。非君好事者，谁来顾寂寞。\"\n",
    "# tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '<|endoftext|>'})\n",
    "print(1)\n",
    "outputs = tokenizer(\n",
    "    data + tokenizer.eos_token,\n",
    "    truncation=True,\n",
    "    max_length=125,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n"
   ],
   "id": "c31e44b463dd1036",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:13:52.505148Z",
     "start_time": "2024-12-06T11:13:52.496541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 可以发现实际上对诗词进行tokenization时，token数和文本数基本是等长的，编码 21128 是 <|endoftext|>\n",
    "# print(outputs)\n",
    "print(outputs[\"input_ids\"][0][1:-1])\n",
    "print(len(data)+1)  # <|endoftext|>: 1\n",
    "# print(outputs[\"attention_mask\"])\n",
    "# print(outputs[\"token_type_ids\"])"
   ],
   "id": "79ddecd7f8beebb9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6133, 1401, 1914, 678, 6810, 8024, 5387, 2495, 5464, 5632, 2428, 511, 1736, 2416, 3188, 5697, 3766, 8024, 4170, 3250, 4958, 4079, 3788, 511, 7312, 2233, 1075, 100, 100, 8024, 2127, 5162, 4491, 5878, 5977, 511, 7582, 7779, 3189, 6139, 5450, 8024, 1094, 2372, 771, 2178, 5862, 511, 7471, 5726, 2347, 4495, 6662, 8024, 5344, 5035, 1993, 1146, 100, 511, 1911, 3698, 678, 6898, 7346, 8024, 2544, 7599, 1220, 100, 5946, 511, 5770, 4371, 5679, 6224, 100, 8024, 3336, 7305, 3187, 6435, 100, 511, 7478, 1409, 1962, 752, 5442, 8024, 6443, 3341, 7560, 2163, 2174, 511, 21128]\n",
      "97\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:13:52.505148Z",
     "start_time": "2024-12-06T11:13:52.496541Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6133, 1401, 1914, 678, 6810, 8024, 5387, 2495, 5464, 5632, 2428, 511, 1736, 2416, 3188, 5697, 3766, 8024, 4170, 3250, 4958, 4079, 3788, 511, 7312, 2233, 1075, 100, 100, 8024, 2127, 5162, 4491, 5878, 5977, 511, 7582, 7779, 3189, 6139, 5450, 8024, 1094, 2372, 771, 2178, 5862, 511, 7471, 5726, 2347, 4495, 6662, 8024, 5344, 5035, 1993, 1146, 100, 511, 1911, 3698, 678, 6898, 7346, 8024, 2544, 7599, 1220, 100, 5946, 511, 5770, 4371, 5679, 6224, 100, 8024, 3336, 7305, 3187, 6435, 100, 511, 7478, 1409, 1962, 752, 5442, 8024, 6443, 3341, 7560, 2163, 2174, 511, 21128]\n",
      "97\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "# 可以发现实际上对诗词进行tokenization时，token数和文本数基本是等长的，编码 21128 是 <|endoftext|>\n",
    "# print(outputs)\n",
    "print(outputs[\"input_ids\"][0][1:-1])\n",
    "print(len(data)+1)  # <|endoftext|>: 1\n",
    "# print(outputs[\"attention_mask\"])\n",
    "# print(outputs[\"token_type_ids\"])"
   ],
   "id": "58eab1d995359d18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 解码测试\n",
    "text = tokenizer.decode(outputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(outputs[\"input_ids\"][0])\n",
    "# 打印解码后的文本\n",
    "print(text)"
   ],
   "id": "ab1204fd9492a449",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.encode(\"谁 来 顾 寂 寞\")",
   "id": "9eb83f65ce9838b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test = tokenizer(\n",
    "        data,\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_tensors=\"np\"\n",
    "    )[\"input_ids\"]\n",
    "test"
   ],
   "id": "24ab950153d8cf40",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
