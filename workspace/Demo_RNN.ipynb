{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817dcb9-150c-4739-8906-c4ea6a7c0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130ff9c-052e-46c6-8ea2-a4778a3a0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用的paddlenlp的word embeddings，具体看 https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/model_zoo/embeddings.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0106c12-fd69-4535-a2e4-6052eedd7b77",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "3784046c-ead1-4607-a8bb-5e6d5f049d8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T06:26:37.180140Z",
     "start_time": "2024-12-03T06:26:34.023397Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:23:07.805638Z",
     "start_time": "2024-12-04T02:23:06.440026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from utils.file_utils import load_base_config, get_project_base_directory"
   ],
   "id": "14c55c5b639779bc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config = load_base_config(\"conf/conf.yaml\")\n",
    "\n",
    "save_path = str(config[\"EMBEDDING_MODEL\"][\"EMBEDDING_MODEL_PATH\"])\n",
    "\n",
    "model_dir = snapshot_download(repo_id=config[\"EMBEDDING_MODEL\"][\"REPO_ID\"],\n",
    "                              local_dir=os.path.join(get_project_base_directory(),\n",
    "                                                     save_path))\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_dir,\n",
    "                                        model_kwargs={\n",
    "                                            'device': 'cpu'},  # 'cuda' if torch.cuda.is_available() else\n",
    "                                        encode_kwargs={'normalize_embeddings': True})"
   ],
   "id": "3b59855b2da4a496"
  },
  {
   "cell_type": "markdown",
   "id": "6c2914da-2182-40da-9fb7-70d7c010a489",
   "metadata": {},
   "source": [
    "### 1.1 Load and Read"
   ]
  },
  {
   "cell_type": "code",
   "id": "093e5639-731c-4fab-bbe0-f5e15642b37a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:19:16.628531Z",
     "start_time": "2024-12-03T07:12:35.608060Z"
    }
   },
   "source": [
    "tr_in = open(\"../couplet/train/in.txt\",encoding='utf8').read()\n",
    "tr_out = open(\"../couplet/train/out.txt\",encoding='utf8').read()\n",
    "te_in = open(\"../couplet/test/in.txt\",encoding='utf8').read()\n",
    "te_out = open(\"../couplet/test/out.txt\",encoding='utf8').read()\n",
    "\n",
    "from paddlenlp.embeddings import TokenEmbedding, list_embedding_name\n",
    "token_embedding = TokenEmbedding(embedding_name=\"w2v.baidu_encyclopedia.target.word-word.dim300\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 694483/694483 [06:23<00:00, 1811.86it/s]\n",
      "\u001B[32m[2024-12-03 15:19:07,768] [    INFO]\u001B[0m - Loading token embedding...\u001B[0m\n",
      "\u001B[32m[2024-12-03 15:19:16,565] [    INFO]\u001B[0m - Finish loading embedding vector.\u001B[0m\n",
      "\u001B[32m[2024-12-03 15:19:16,568] [    INFO]\u001B[0m - Token Embedding info:             \n",
      "Unknown index: 635963             \n",
      "Unknown token: [UNK]             \n",
      "Padding index: 635964             \n",
      "Padding token: [PAD]             \n",
      "Shape :[635965, 300]\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 Vocabulary",
   "id": "99889536-2da6-40d5-8a2a-0cacf597f7db"
  },
  {
   "cell_type": "code",
   "id": "4646dab9-b2c2-4cbd-997b-4210e78dd01e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:19:38.976883Z",
     "start_time": "2024-12-03T07:19:37.135189Z"
    }
   },
   "source": [
    "vocab = list(set(tr_in + tr_out + te_in + te_out))\n",
    "vocab.insert(0,'<EOS>')\n",
    "vocab.insert(0,'<BOS>')\n",
    "vocab.insert(0,'<PAD>')\n",
    "\n",
    "embeddings = dict()\n",
    "embeddings[2] = np.random.rand(300,).astype('float32') *2 - 1 # range(-1:1)\n",
    "embeddings[1] = np.random.rand(300,).astype('float32') *2 - 1 # range(-1:1)\n",
    "embeddings[0] = np.zeros(300,)\n",
    "\n",
    "for i,w in enumerate(vocab,start = 3):\n",
    "    embeddings[i] = token_embedding.search(w).reshape(300,)\n",
    "    \n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx2word = {i:w for i,w in enumerate(vocab)}"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "c10eb1bc-ce17-4a3b-b107-e7be8aa755b5",
   "metadata": {},
   "source": [
    "### 1.3 Word2idx"
   ]
  },
  {
   "cell_type": "code",
   "id": "63becea8-5c83-479c-ac34-32e180105df6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:19:54.072299Z",
     "start_time": "2024-12-03T07:19:46.666936Z"
    }
   },
   "source": [
    "def convert(x, y, word2id):\n",
    "    in_sentences = [[word2id[w] for w in sent.split()] for sent in x]\n",
    "    out_sentences = [[word2id[w] for w in sent.split()] for sent in y]\n",
    "    \n",
    "    # 根据句子的长度排序\n",
    "    #sorted_len = sorted([ (i,len(x)) for i,x in enumerate(tr_in.split('\\n')[:-3000])], key=lambda x: x[1])\n",
    "    \n",
    "    #sorted_index = [x[0] for x in sorted_len]\n",
    "    #in_sentences = [in_sentences[i] for i in sorted_index]\n",
    "    #out_sentences = [out_sentences[i] for i in sorted_index]\n",
    "    \n",
    "    return in_sentences, out_sentences\n",
    "\n",
    "train_x, train_y = convert(tr_in.split('\\n')[:-3000], tr_out.split('\\n')[:-3000], word2idx)\n",
    "dev_x, dev_y = convert(tr_in.split('\\n')[-3000:], tr_out.split('\\n')[-3000:], word2idx)\n",
    "test_x, test_y = convert(te_in.split('\\n')[:-1], te_out.split('\\n')[:-1], word2idx)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "421b7e40-55a5-421b-9ec9-ced329a5d90a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:19:58.789648Z",
     "start_time": "2024-12-03T07:19:58.755334Z"
    }
   },
   "source": [
    "train_x[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3778, 1029, 6167, 6686, 6686, 8662, 5181]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "7818d3e1-f50b-4b70-a183-1519e74993ce",
   "metadata": {},
   "source": [
    "### 1.4 Batch"
   ]
  },
  {
   "cell_type": "code",
   "id": "bb822937-b866-479a-8c23-7555ea577b44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:24:59.123953Z",
     "start_time": "2024-12-03T07:24:55.879986Z"
    }
   },
   "source": [
    "# 这个函数的作用是我们输入训练集的样本个数， batch_size大小， 就会返回多批 连续的batch_size个索引， 每一个索引代表一个样本\n",
    "# 也就是可以根据这个索引去拿到一个个的batch\n",
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx+minibatch_size, n)))\n",
    "    return minibatches      # 这个会返回多批连着的bath_size个索引  \n",
    "#get_minibatches(len(train_en), 32)\n",
    "\n",
    "# 这个函数是在做数据预处理， 由于每个句子都不是一样长， 所以通过这个函数就可以把句子进行补齐， 不够长的在句子后面添加0\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]    # 得到每个句子的长度\n",
    "    n_samples = len(seqs)       # 得到一共有多少个句子\n",
    "    max_len = np.max(lengths)              # 找出最大的句子长度\n",
    "    \n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')    # 按照最大句子长度生成全0矩阵\n",
    "    x_lengths = np.array(lengths).astype('int32')\n",
    "    for idx, seq in enumerate(seqs):        # 把有句子的位置填充进去\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x, x_lengths      # x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)   # 得到batch个索引\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:   # 每批数据的索引\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]   # 取数据\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]  # 取数据\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences) # 填充成一样的长度， 但是要记录一下句子的真实长度， 这个在后面输入网络的时候得用\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return all_ex\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_x, train_y, batch_size)   # 产生训练集\n",
    "random.shuffle(train_data)\n",
    "dev_data = gen_examples(dev_x, dev_y, batch_size)   # 产生验证集"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "6d65a153-fcf6-4f43-aa11-6ccb168ba4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:25:00.660584Z",
     "start_time": "2024-12-03T07:25:00.627020Z"
    }
   },
   "source": [
    "train_data[0][0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3468, 3907, 1936, ...,    0,    0,    0],\n",
       "       [7392, 6289,  667, ...,    0,    0,    0],\n",
       "       [7345, 5558, 1352, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 784, 5629, 3424, ..., 6542, 7493, 3218],\n",
       "       [ 393, 5868, 1338, ...,    0,    0,    0],\n",
       "       [5358, 2492, 3381, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "e239c03f-b606-4067-b3d6-2801bac1f843",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7071aa-827d-4752-ba1d-4506a5def66f",
   "metadata": {},
   "source": [
    "### 2.1 Encoder(GRU)"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc0556a1-723e-4b70-a2e8-a5ec63894d05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:30:19.035377Z",
     "start_time": "2024-12-03T07:30:19.017346Z"
    }
   },
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size*2, dec_hidden_size)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))   # [batch_size, seq_len, embed_size]\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # [batch_size, seq_len, 2*enc_hidden_size]\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()   # [batch_size, seq_len, 2*enc_hidden_size]\n",
    "        hid = hid[:, original_idx.long()].contiguous()   # [2, batch_size, enc_hidden_size]\n",
    "        \n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1)   # 双向的GRU， 这里是最后一个状态， 联结起来  [batch_size, 2*enc_hidden_size]\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)  # [1, batch_size, dec_hidden_size]\n",
    "        \n",
    "        return out, hid\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "5b220048-687b-47d3-94c1-efa2d1d8590d",
   "metadata": {},
   "source": [
    "### 2.2 Attention"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc1f6684-02c3-42a0-922f-3e4ba87372af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:30:22.569638Z",
     "start_time": "2024-12-03T07:30:22.556764Z"
    }
   },
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        \n",
    "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size*2+dec_hidden_size, dec_hidden_size)\n",
    "    \n",
    "    def forward(self, output, encoder_output, mask):\n",
    "        # output: [batch_size, seq_len_y-1, dec_hidden_size]  这个output 是decoder的每个时间步输出的隐藏状态\n",
    "        # encoder_output: [batch_size, seq_len_x, 2*enc_hidden_size]\n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = encoder_output.size(1)\n",
    "        \n",
    "        context_in = self.linear_in(encoder_output.view(batch_size*input_len, -1))  # [batch_size*seq_len_x,dec_hidden_size]\n",
    "        context_in = context_in.view(batch_size, input_len, -1)  # [batch_size, seq_len_x, dec_hidden_size]\n",
    "        context_in = context_in.transpose(1, 2)   # [batch_size, dec_hidden_size, seq_len_x]\n",
    "        \n",
    "        attn = torch.bmm(output, context_in)  # [batch_size, seq_len_y-1, seq_len_x]\n",
    "        # 这个东西就是求得当前时间步的输出output和所有输入相似性关系的一个得分score , 下面就是通过softmax把这个得分转成权重\n",
    "        attn = F.softmax(attn, dim=2)    # 此时第二维度的数字全都变成了0-1之间的数， 越大表示当前的输出output与哪个相关程度越大\n",
    "        \n",
    "        context = torch.bmm(attn, encoder_output)   # [batch_size, seq_len_y-1, 2*enc_hidden_size]\n",
    "        \n",
    "        output = torch.cat((context, output), dim=2)  # [batch_size, seq_len_y-1, 2*enc_hidden_size+dec_hidden_size]\n",
    "        \n",
    "        output = output.view(batch_size*output_len, -1)   # [batch_size*seq_len_y-1, 2*enc_hidden_size+dec_hidden_size]\n",
    "        output = torch.tanh(self.linear_out(output))     # [batch_size*seq_len_y-1, dec_hidden_size]\n",
    "        output = output.view(batch_size, output_len, -1)  # [batch_size, seq_len_y-1, dec_hidden_size]\n",
    "        \n",
    "        return output, attn\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "37b6917b-7b35-4bbb-a7bc-dbb1304c9523",
   "metadata": {},
   "source": [
    "### 2.3 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "id": "b7c02c73-0247-4605-b2cf-9db82418ef71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:30:25.762631Z",
     "start_time": "2024-12-03T07:30:25.746115Z"
    }
   },
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def create_mask(self, x_len, y_len):\n",
    "        # a mask of shape x_len*y_len\n",
    "        x_mask = torch.arange(x_len.max(), device=x_len.device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(y_len.max(), device=x_len.device)[None, :] < y_len[:, None]\n",
    "        \n",
    "        x_mask = x_mask.float()\n",
    "        y_mask = y_mask.float()\n",
    "        mask = (1 - x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, encoder_out, encoder_out_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]   # 句子从长到短排序\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        \n",
    "        y_sorted = self.dropout(self.embed(y_sorted))     # [batch_size, output_length, embed_size]\n",
    "        \n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()   # [batch_size, seq_len_y-1, dec_hidden_size]\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        mask = self.create_mask(y_lengths, encoder_out_lengths)\n",
    "        \n",
    "        output, attn = self.attention(output_seq, encoder_out, mask)\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "        \n",
    "        return output, hid, attn\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "0f0c4daa-583f-4066-b886-5585deb7ccb2",
   "metadata": {},
   "source": [
    "### 2.4 Wrapped Network"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8b50c53-cbdf-4b14-8d1d-72508a063c2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:30:34.356239Z",
     "start_time": "2024-12-03T07:30:34.348665Z"
    }
   },
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(encoder_out, x_lengths, y, y_lengths, hid)\n",
    "        \n",
    "        return output, attn\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(encoder_out, x_lengths, y, torch.ones(batch_size).long().to(y.device), hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        \n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "6e43935b-2a2e-49b3-9a56-b63dd7c95720",
   "metadata": {},
   "source": [
    "### 2.5 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "id": "16276c9b-ae8a-4b00-b9f6-2f4ccf507694",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:30:36.967016Z",
     "start_time": "2024-12-03T07:30:36.959713Z"
    }
   },
   "source": [
    "# masked cross entropy loss\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "    \n",
    "    def forward(self, input, target, mask):\n",
    "        # input: [batch_size, seq_len, vocab_size]    每个单词的可能性\n",
    "        input = input.contiguous().view(-1, input.size(2))   # [batch_size*seq_len-1, vocab_size]\n",
    "        target = target.contiguous().view(-1, 1)    #  [batch_size*seq_len-1, 1]\n",
    "        \n",
    "        mask = mask.contiguous().view(-1, 1)   # [batch_size*seq_len-1, 1]\n",
    "        output = -input.gather(1, target) * mask # 在每个vocab_size维度取正确单词的索引， 但是里面有很多是填充进去的， 所以mask去掉这些填充的\n",
    "        # 这个其实在写一个NLloss ， 也就是sortmax的取负号\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "        \n",
    "        return output  # [batch_size*seq_len-1, 1]\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "9e72c1ed-622e-4902-854c-6512de61fc86",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a73ce-1550-478e-ab16-3d5820848141",
   "metadata": {},
   "source": [
    "### Config Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "b41998b5-ee75-4be1-9a9f-4007a9fd22e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:31:44.411831Z",
     "start_time": "2024-12-03T07:31:42.642663Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "encoder = Encoder(vocab_size=len(word2idx), enc_hidden_size=hidden_size, dec_hidden_size=hidden_size, embed_size = 300, dropout=dropout)\n",
    "decoder = Decoder(vocab_size=len(word2idx), enc_hidden_size=hidden_size, dec_hidden_size=hidden_size, embed_size = 300, dropout=dropout)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "2240802e-7787-45d0-97f8-ef6e1f6c4fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T07:31:50.035015Z",
     "start_time": "2024-12-03T07:31:50.030341Z"
    }
   },
   "source": [
    "train_data[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3468, 3907, 1936, ...,    0,    0,    0],\n",
       "        [7392, 6289,  667, ...,    0,    0,    0],\n",
       "        [7345, 5558, 1352, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 784, 5629, 3424, ..., 6542, 7493, 3218],\n",
       "        [ 393, 5868, 1338, ...,    0,    0,    0],\n",
       "        [5358, 2492, 3381, ...,    0,    0,    0]]),\n",
       " array([ 7, 10,  7,  7, 12, 17,  4, 13,  7, 13,  9, 13, 10, 12,  3, 12,  7,\n",
       "        12, 17,  7,  7, 15, 17,  7,  7, 12,  2,  7,  7, 22,  3, 12,  7, 18,\n",
       "         4,  7,  7,  7,  7,  7,  7,  7,  7, 12,  7,  7,  7, 11, 12,  2,  5,\n",
       "         7,  7,  7,  7,  5,  7,  7,  5,  7,  7, 27,  7, 13]),\n",
       " array([[ 398, 5353, 3814, ...,    0,    0,    0],\n",
       "        [5178, 8033, 6866, ...,    0,    0,    0],\n",
       "        [7067, 7224, 7308, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [2659, 3479, 7906, ..., 3332, 8250, 8127],\n",
       "        [7289, 7345, 1706, ...,    0,    0,    0],\n",
       "        [5345, 8053, 8250, ...,    0,    0,    0]]),\n",
       " array([ 7, 10,  7,  7, 12, 17,  4, 13,  7, 13,  9, 13, 10, 12,  3, 12,  7,\n",
       "        12, 17,  7,  7, 15, 17,  7,  7, 12,  2,  7,  7, 22,  3, 12,  7, 18,\n",
       "         4,  7,  7,  7,  7,  7,  7,  7,  7, 12,  7,  7,  7, 11, 12,  2,  5,\n",
       "         7,  7,  7,  7,  5,  7,  7,  5,  7,  7, 27,  7, 13]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T08:30:24.243661Z",
     "start_time": "2024-12-03T08:30:23.890695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 检查数据集\n",
    "def validate_data(data):\n",
    "    valid_data = []\n",
    "    for mb_x, mb_x_len, mb_y, mb_y_len in data:\n",
    "        if all(length > 0 for length in mb_x_len) and all(length > 1 for length in mb_y_len):\n",
    "            valid_data.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return valid_data\n",
    "\n",
    "train_data = validate_data(train_data)\n",
    "dev_data = validate_data(dev_data)"
   ],
   "id": "c8ae3777661b3dd",
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "ac82b9d5-8003-4532-a375-8a7a5f8753ec",
   "metadata": {},
   "source": [
    "#### Train and evluate"
   ]
  },
  {
   "cell_type": "code",
   "id": "80d939b2-358e-4cb7-8879-fd2284754791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T08:58:22.202433Z",
     "start_time": "2024-12-03T08:30:27.093587Z"
    }
   },
   "source": [
    "# 定义训练和验证函数\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()    # 这个是一个batch的英文句子 大小是[batch_size, seq_len]\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()    # 每个句子的长度\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()  # 解码器那边的输入， 输入一个单词去预测另外一个单词\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()   # 解码器那边的输出  [batch_size, seq_len-1]\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()  # 这个减去1， 因为没有了最后一个  [batch_size, seq_len-1]\n",
    "            mb_y_len[mb_y_len<=0] =  1   # 这句话是为了以防出错\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]  \n",
    "            # [batch_size, mb_y_len.max()], 上面是bool类型， 下面是float类型， 只计算每个句子的有效部分， 填充的那部分去掉\n",
    "            mb_out_mask = mb_out_mask.float()  # [batch_size, seq_len-1]  因为mb_y_len.max()就是seq_len-1\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "    print('Evaluation loss', total_loss / total_num_words)\n",
    "\n",
    "def train(model, data, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in  enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "            # 更新\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)     # 这里防止梯度爆炸， 这是和以往不太一样的地方\n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print('Epoch', epoch, 'iteration', it, 'loss', loss.item())\n",
    "\n",
    "        print('Epoch', epoch, 'Training loss', total_loss / total_num_words)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data)\n",
    "        \n",
    "# 训练\n",
    "train(model, train_data, num_epochs=20)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 4.52675724029541\n",
      "Epoch 0 iteration 100 loss 4.917457580566406\n",
      "Epoch 0 iteration 200 loss 4.855518341064453\n",
      "Epoch 0 iteration 300 loss 4.770705223083496\n",
      "Epoch 0 iteration 400 loss 4.601000785827637\n",
      "Epoch 0 iteration 500 loss 4.659682750701904\n",
      "Epoch 0 iteration 600 loss 4.720873832702637\n",
      "Epoch 0 iteration 700 loss 4.427868366241455\n",
      "Epoch 0 iteration 800 loss 4.743442058563232\n",
      "Epoch 0 iteration 900 loss 4.75640869140625\n",
      "Epoch 0 iteration 1000 loss 4.795638084411621\n",
      "Epoch 0 iteration 1100 loss 4.933182239532471\n",
      "Epoch 0 iteration 1200 loss 4.194342136383057\n",
      "Epoch 0 iteration 1300 loss 4.715646266937256\n",
      "Epoch 0 iteration 1400 loss 4.6921491622924805\n",
      "Epoch 0 iteration 1500 loss 4.713183403015137\n",
      "Epoch 0 iteration 1600 loss 4.4577555656433105\n",
      "Epoch 0 iteration 1700 loss 4.588473796844482\n",
      "Epoch 0 iteration 1800 loss 4.564268589019775\n",
      "Epoch 0 iteration 1900 loss 4.565576076507568\n",
      "Epoch 0 iteration 2000 loss 4.558114051818848\n",
      "Epoch 0 iteration 2100 loss 4.564469337463379\n",
      "Epoch 0 iteration 2200 loss 4.690690994262695\n",
      "Epoch 0 iteration 2300 loss 4.7068939208984375\n",
      "Epoch 0 iteration 2400 loss 4.413317680358887\n",
      "Epoch 0 iteration 2500 loss 4.57799768447876\n",
      "Epoch 0 iteration 2600 loss 4.5517778396606445\n",
      "Epoch 0 iteration 2700 loss 4.576460361480713\n",
      "Epoch 0 iteration 2800 loss 4.489599704742432\n",
      "Epoch 0 iteration 2900 loss 4.542547702789307\n",
      "Epoch 0 iteration 3000 loss 4.86298131942749\n",
      "Epoch 0 iteration 3100 loss 4.649747371673584\n",
      "Epoch 0 iteration 3200 loss 4.483824729919434\n",
      "Epoch 0 iteration 3300 loss 4.566288948059082\n",
      "Epoch 0 iteration 3400 loss 4.60819673538208\n",
      "Epoch 0 iteration 3500 loss 4.468106746673584\n",
      "Epoch 0 iteration 3600 loss 4.26645040512085\n",
      "Epoch 0 iteration 3700 loss 4.690692901611328\n",
      "Epoch 0 iteration 3800 loss 4.435907363891602\n",
      "Epoch 0 iteration 3900 loss 4.940751552581787\n",
      "Epoch 0 iteration 4000 loss 4.479783058166504\n",
      "Epoch 0 iteration 4100 loss 4.530648708343506\n",
      "Epoch 0 iteration 4200 loss 5.001981735229492\n",
      "Epoch 0 iteration 4300 loss 4.556782245635986\n",
      "Epoch 0 iteration 4400 loss 4.540895462036133\n",
      "Epoch 0 iteration 4500 loss 4.4809441566467285\n",
      "Epoch 0 iteration 4600 loss 4.722322940826416\n",
      "Epoch 0 iteration 4700 loss 4.56080436706543\n",
      "Epoch 0 iteration 4800 loss 4.672322750091553\n",
      "Epoch 0 iteration 4900 loss 4.582774639129639\n",
      "Epoch 0 iteration 5000 loss 4.749802589416504\n",
      "Epoch 0 iteration 5100 loss 4.599877834320068\n",
      "Epoch 0 iteration 5200 loss 4.562156677246094\n",
      "Epoch 0 iteration 5300 loss 4.5171098709106445\n",
      "Epoch 0 iteration 5400 loss 4.359279632568359\n",
      "Epoch 0 iteration 5500 loss 4.555453300476074\n",
      "Epoch 0 iteration 5600 loss 4.6389546394348145\n",
      "Epoch 0 iteration 5700 loss 4.4279561042785645\n",
      "Epoch 0 iteration 5800 loss 4.543641567230225\n",
      "Epoch 0 iteration 5900 loss 4.236008167266846\n",
      "Epoch 0 iteration 6000 loss 4.571290969848633\n",
      "Epoch 0 iteration 6100 loss 4.481481075286865\n",
      "Epoch 0 iteration 6200 loss 4.493033409118652\n",
      "Epoch 0 iteration 6300 loss 4.331146240234375\n",
      "Epoch 0 iteration 6400 loss 4.6331305503845215\n",
      "Epoch 0 iteration 6500 loss 4.70310640335083\n",
      "Epoch 0 iteration 6600 loss 4.523108005523682\n",
      "Epoch 0 iteration 6700 loss 4.5609211921691895\n",
      "Epoch 0 iteration 6800 loss 4.419071674346924\n",
      "Epoch 0 iteration 6900 loss 4.510962963104248\n",
      "Epoch 0 iteration 7000 loss 4.483887672424316\n",
      "Epoch 0 iteration 7100 loss 4.669241428375244\n",
      "Epoch 0 iteration 7200 loss 4.554616928100586\n",
      "Epoch 0 iteration 7300 loss 4.431420803070068\n",
      "Epoch 0 iteration 7400 loss 4.3689751625061035\n",
      "Epoch 0 iteration 7500 loss 4.357762336730957\n",
      "Epoch 0 iteration 7600 loss 4.78739070892334\n",
      "Epoch 0 iteration 7700 loss 4.433958053588867\n",
      "Epoch 0 iteration 7800 loss 4.5316667556762695\n",
      "Epoch 0 iteration 7900 loss 4.291994571685791\n",
      "Epoch 0 iteration 8000 loss 4.308803081512451\n",
      "Epoch 0 iteration 8100 loss 4.6788201332092285\n",
      "Epoch 0 iteration 8200 loss 4.730970859527588\n",
      "Epoch 0 iteration 8300 loss 4.714268684387207\n",
      "Epoch 0 iteration 8400 loss 4.533039093017578\n",
      "Epoch 0 iteration 8500 loss 4.430197715759277\n",
      "Epoch 0 iteration 8600 loss 4.134418487548828\n",
      "Epoch 0 iteration 8700 loss 4.5345234870910645\n",
      "Epoch 0 iteration 8800 loss 4.514768123626709\n",
      "Epoch 0 iteration 8900 loss 4.539791107177734\n",
      "Epoch 0 iteration 9000 loss 4.489532470703125\n",
      "Epoch 0 iteration 9100 loss 4.611903667449951\n",
      "Epoch 0 iteration 9200 loss 4.566237926483154\n",
      "Epoch 0 iteration 9300 loss 4.418603897094727\n",
      "Epoch 0 iteration 9400 loss 4.364651679992676\n",
      "Epoch 0 iteration 9500 loss 4.516185760498047\n",
      "Epoch 0 iteration 9600 loss 4.4427337646484375\n",
      "Epoch 0 iteration 9700 loss 4.5117034912109375\n",
      "Epoch 0 iteration 9800 loss 4.569661617279053\n",
      "Epoch 0 Training loss 4.550662053388456\n",
      "Evaluation loss 4.432149870514271\n",
      "Epoch 1 iteration 0 loss 4.249419689178467\n",
      "Epoch 1 iteration 100 loss 4.670160293579102\n",
      "Epoch 1 iteration 200 loss 4.578043460845947\n",
      "Epoch 1 iteration 300 loss 4.623025894165039\n",
      "Epoch 1 iteration 400 loss 4.312684535980225\n",
      "Epoch 1 iteration 500 loss 4.458136558532715\n",
      "Epoch 1 iteration 600 loss 4.521172523498535\n",
      "Epoch 1 iteration 700 loss 4.230521202087402\n",
      "Epoch 1 iteration 800 loss 4.5662760734558105\n",
      "Epoch 1 iteration 900 loss 4.607515335083008\n",
      "Epoch 1 iteration 1000 loss 4.569401264190674\n",
      "Epoch 1 iteration 1100 loss 4.76374626159668\n",
      "Epoch 1 iteration 1200 loss 4.001948356628418\n",
      "Epoch 1 iteration 1300 loss 4.552548885345459\n",
      "Epoch 1 iteration 1400 loss 4.506577014923096\n",
      "Epoch 1 iteration 1500 loss 4.507868766784668\n",
      "Epoch 1 iteration 1600 loss 4.3226470947265625\n",
      "Epoch 1 iteration 1700 loss 4.400784969329834\n",
      "Epoch 1 iteration 1800 loss 4.317628383636475\n",
      "Epoch 1 iteration 1900 loss 4.4022393226623535\n",
      "Epoch 1 iteration 2000 loss 4.383214473724365\n",
      "Epoch 1 iteration 2100 loss 4.402495861053467\n",
      "Epoch 1 iteration 2200 loss 4.5397515296936035\n",
      "Epoch 1 iteration 2300 loss 4.517645835876465\n",
      "Epoch 1 iteration 2400 loss 4.26087760925293\n",
      "Epoch 1 iteration 2500 loss 4.366937637329102\n",
      "Epoch 1 iteration 2600 loss 4.376765251159668\n",
      "Epoch 1 iteration 2700 loss 4.367311000823975\n",
      "Epoch 1 iteration 2800 loss 4.284696578979492\n",
      "Epoch 1 iteration 2900 loss 4.33774995803833\n",
      "Epoch 1 iteration 3000 loss 4.615163326263428\n",
      "Epoch 1 iteration 3100 loss 4.40270471572876\n",
      "Epoch 1 iteration 3200 loss 4.36846923828125\n",
      "Epoch 1 iteration 3300 loss 4.362261772155762\n",
      "Epoch 1 iteration 3400 loss 4.346338272094727\n",
      "Epoch 1 iteration 3500 loss 4.280235290527344\n",
      "Epoch 1 iteration 3600 loss 4.15755558013916\n",
      "Epoch 1 iteration 3700 loss 4.513617038726807\n",
      "Epoch 1 iteration 3800 loss 4.339809894561768\n",
      "Epoch 1 iteration 3900 loss 4.850061893463135\n",
      "Epoch 1 iteration 4000 loss 4.327030181884766\n",
      "Epoch 1 iteration 4100 loss 4.429572582244873\n",
      "Epoch 1 iteration 4200 loss 4.871119499206543\n",
      "Epoch 1 iteration 4300 loss 4.4127068519592285\n",
      "Epoch 1 iteration 4400 loss 4.35786247253418\n",
      "Epoch 1 iteration 4500 loss 4.372146129608154\n",
      "Epoch 1 iteration 4600 loss 4.571773529052734\n",
      "Epoch 1 iteration 4700 loss 4.416059970855713\n",
      "Epoch 1 iteration 4800 loss 4.513430595397949\n",
      "Epoch 1 iteration 4900 loss 4.468862056732178\n",
      "Epoch 1 iteration 5000 loss 4.582127094268799\n",
      "Epoch 1 iteration 5100 loss 4.430355072021484\n",
      "Epoch 1 iteration 5200 loss 4.373322486877441\n",
      "Epoch 1 iteration 5300 loss 4.389444828033447\n",
      "Epoch 1 iteration 5400 loss 4.254502296447754\n",
      "Epoch 1 iteration 5500 loss 4.384982109069824\n",
      "Epoch 1 iteration 5600 loss 4.4938130378723145\n",
      "Epoch 1 iteration 5700 loss 4.315229415893555\n",
      "Epoch 1 iteration 5800 loss 4.438264846801758\n",
      "Epoch 1 iteration 5900 loss 4.071371555328369\n",
      "Epoch 1 iteration 6000 loss 4.489150047302246\n",
      "Epoch 1 iteration 6100 loss 4.342151641845703\n",
      "Epoch 1 iteration 6200 loss 4.395637512207031\n",
      "Epoch 1 iteration 6300 loss 4.2311482429504395\n",
      "Epoch 1 iteration 6400 loss 4.435997486114502\n",
      "Epoch 1 iteration 6500 loss 4.615202903747559\n",
      "Epoch 1 iteration 6600 loss 4.368155479431152\n",
      "Epoch 1 iteration 6700 loss 4.434421539306641\n",
      "Epoch 1 iteration 6800 loss 4.3366193771362305\n",
      "Epoch 1 iteration 6900 loss 4.395750522613525\n",
      "Epoch 1 iteration 7000 loss 4.367537975311279\n",
      "Epoch 1 iteration 7100 loss 4.437835693359375\n",
      "Epoch 1 iteration 7200 loss 4.513186931610107\n",
      "Epoch 1 iteration 7300 loss 4.326860427856445\n",
      "Epoch 1 iteration 7400 loss 4.223979473114014\n",
      "Epoch 1 iteration 7500 loss 4.225578784942627\n",
      "Epoch 1 iteration 7600 loss 4.63087272644043\n",
      "Epoch 1 iteration 7700 loss 4.384069442749023\n",
      "Epoch 1 iteration 7800 loss 4.3903679847717285\n",
      "Epoch 1 iteration 7900 loss 4.176756858825684\n",
      "Epoch 1 iteration 8000 loss 4.1658935546875\n",
      "Epoch 1 iteration 8100 loss 4.590130805969238\n",
      "Epoch 1 iteration 8200 loss 4.655699253082275\n",
      "Epoch 1 iteration 8300 loss 4.597620010375977\n",
      "Epoch 1 iteration 8400 loss 4.398993015289307\n",
      "Epoch 1 iteration 8500 loss 4.256478786468506\n",
      "Epoch 1 iteration 8600 loss 4.054211139678955\n",
      "Epoch 1 iteration 8700 loss 4.44390869140625\n",
      "Epoch 1 iteration 8800 loss 4.475432872772217\n",
      "Epoch 1 iteration 8900 loss 4.462771892547607\n",
      "Epoch 1 iteration 9000 loss 4.336450099945068\n",
      "Epoch 1 iteration 9100 loss 4.42531681060791\n",
      "Epoch 1 iteration 9200 loss 4.491481781005859\n",
      "Epoch 1 iteration 9300 loss 4.358654022216797\n",
      "Epoch 1 iteration 9400 loss 4.218417644500732\n",
      "Epoch 1 iteration 9500 loss 4.4587321281433105\n",
      "Epoch 1 iteration 9600 loss 4.362987518310547\n",
      "Epoch 1 iteration 9700 loss 4.426447868347168\n",
      "Epoch 1 iteration 9800 loss 4.449743270874023\n",
      "Epoch 1 Training loss 4.404642160945103\n",
      "Epoch 2 iteration 0 loss 4.224893569946289\n",
      "Epoch 2 iteration 100 loss 4.5414838790893555\n",
      "Epoch 2 iteration 200 loss 4.433922290802002\n",
      "Epoch 2 iteration 300 loss 4.518270969390869\n",
      "Epoch 2 iteration 400 loss 4.24944543838501\n",
      "Epoch 2 iteration 500 loss 4.360758304595947\n",
      "Epoch 2 iteration 600 loss 4.388726711273193\n",
      "Epoch 2 iteration 700 loss 4.142613887786865\n",
      "Epoch 2 iteration 800 loss 4.482404708862305\n",
      "Epoch 2 iteration 900 loss 4.482598304748535\n",
      "Epoch 2 iteration 1000 loss 4.477334499359131\n",
      "Epoch 2 iteration 1100 loss 4.609078407287598\n",
      "Epoch 2 iteration 1200 loss 3.9302141666412354\n",
      "Epoch 2 iteration 1300 loss 4.478936672210693\n",
      "Epoch 2 iteration 1400 loss 4.42024040222168\n",
      "Epoch 2 iteration 1500 loss 4.378689289093018\n",
      "Epoch 2 iteration 1600 loss 4.20576810836792\n",
      "Epoch 2 iteration 1700 loss 4.319018363952637\n",
      "Epoch 2 iteration 1800 loss 4.257725715637207\n",
      "Epoch 2 iteration 1900 loss 4.2914557456970215\n",
      "Epoch 2 iteration 2000 loss 4.340392589569092\n",
      "Epoch 2 iteration 2100 loss 4.294195175170898\n",
      "Epoch 2 iteration 2200 loss 4.465112209320068\n",
      "Epoch 2 iteration 2300 loss 4.378530025482178\n",
      "Epoch 2 iteration 2400 loss 4.166995525360107\n",
      "Epoch 2 iteration 2500 loss 4.31101131439209\n",
      "Epoch 2 iteration 2600 loss 4.277828693389893\n",
      "Epoch 2 iteration 2700 loss 4.289491653442383\n",
      "Epoch 2 iteration 2800 loss 4.196318626403809\n",
      "Epoch 2 iteration 2900 loss 4.209480285644531\n",
      "Epoch 2 iteration 3000 loss 4.586239337921143\n",
      "Epoch 2 iteration 3100 loss 4.355156421661377\n",
      "Epoch 2 iteration 3200 loss 4.228763580322266\n",
      "Epoch 2 iteration 3300 loss 4.315847396850586\n",
      "Epoch 2 iteration 3400 loss 4.273435592651367\n",
      "Epoch 2 iteration 3500 loss 4.153491020202637\n",
      "Epoch 2 iteration 3600 loss 4.0963640213012695\n",
      "Epoch 2 iteration 3700 loss 4.437966346740723\n",
      "Epoch 2 iteration 3800 loss 4.295975208282471\n",
      "Epoch 2 iteration 3900 loss 4.774169445037842\n",
      "Epoch 2 iteration 4000 loss 4.256157875061035\n",
      "Epoch 2 iteration 4100 loss 4.333939075469971\n",
      "Epoch 2 iteration 4200 loss 4.759160995483398\n",
      "Epoch 2 iteration 4300 loss 4.300940990447998\n",
      "Epoch 2 iteration 4400 loss 4.340824127197266\n",
      "Epoch 2 iteration 4500 loss 4.262622833251953\n",
      "Epoch 2 iteration 4600 loss 4.46634578704834\n",
      "Epoch 2 iteration 4700 loss 4.385804653167725\n",
      "Epoch 2 iteration 4800 loss 4.437556266784668\n",
      "Epoch 2 iteration 4900 loss 4.383084774017334\n",
      "Epoch 2 iteration 5000 loss 4.5280070304870605\n",
      "Epoch 2 iteration 5100 loss 4.341493606567383\n",
      "Epoch 2 iteration 5200 loss 4.333235263824463\n",
      "Epoch 2 iteration 5300 loss 4.299931526184082\n",
      "Epoch 2 iteration 5400 loss 4.21598482131958\n",
      "Epoch 2 iteration 5500 loss 4.31194543838501\n",
      "Epoch 2 iteration 5600 loss 4.453372478485107\n",
      "Epoch 2 iteration 5700 loss 4.295752048492432\n",
      "Epoch 2 iteration 5800 loss 4.319137096405029\n",
      "Epoch 2 iteration 5900 loss 3.967714309692383\n",
      "Epoch 2 iteration 6000 loss 4.3720808029174805\n",
      "Epoch 2 iteration 6100 loss 4.285009384155273\n",
      "Epoch 2 iteration 6200 loss 4.349417209625244\n",
      "Epoch 2 iteration 6300 loss 4.113514423370361\n",
      "Epoch 2 iteration 6400 loss 4.4195756912231445\n",
      "Epoch 2 iteration 6500 loss 4.557794094085693\n",
      "Epoch 2 iteration 6600 loss 4.322802543640137\n",
      "Epoch 2 iteration 6700 loss 4.332118988037109\n",
      "Epoch 2 iteration 6800 loss 4.253566265106201\n",
      "Epoch 2 iteration 6900 loss 4.349553108215332\n",
      "Epoch 2 iteration 7000 loss 4.316233158111572\n",
      "Epoch 2 iteration 7100 loss 4.379711151123047\n",
      "Epoch 2 iteration 7200 loss 4.438813209533691\n",
      "Epoch 2 iteration 7300 loss 4.24184513092041\n",
      "Epoch 2 iteration 7400 loss 4.159624099731445\n",
      "Epoch 2 iteration 7500 loss 4.1488752365112305\n",
      "Epoch 2 iteration 7600 loss 4.518200397491455\n",
      "Epoch 2 iteration 7700 loss 4.327051162719727\n",
      "Epoch 2 iteration 7800 loss 4.360977649688721\n",
      "Epoch 2 iteration 7900 loss 4.09867525100708\n",
      "Epoch 2 iteration 8000 loss 4.117759704589844\n",
      "Epoch 2 iteration 8100 loss 4.503712177276611\n",
      "Epoch 2 iteration 8200 loss 4.567492961883545\n",
      "Epoch 2 iteration 8300 loss 4.5088372230529785\n",
      "Epoch 2 iteration 8400 loss 4.319450855255127\n",
      "Epoch 2 iteration 8500 loss 4.174472808837891\n",
      "Epoch 2 iteration 8600 loss 3.946613311767578\n",
      "Epoch 2 iteration 8700 loss 4.356777667999268\n",
      "Epoch 2 iteration 8800 loss 4.3996357917785645\n",
      "Epoch 2 iteration 8900 loss 4.380691051483154\n",
      "Epoch 2 iteration 9000 loss 4.213654518127441\n",
      "Epoch 2 iteration 9100 loss 4.389758110046387\n",
      "Epoch 2 iteration 9200 loss 4.436798095703125\n",
      "Epoch 2 iteration 9300 loss 4.329813480377197\n",
      "Epoch 2 iteration 9400 loss 4.169198989868164\n",
      "Epoch 2 iteration 9500 loss 4.372934341430664\n",
      "Epoch 2 iteration 9600 loss 4.300947189331055\n",
      "Epoch 2 iteration 9700 loss 4.392324924468994\n",
      "Epoch 2 iteration 9800 loss 4.381279468536377\n",
      "Epoch 2 Training loss 4.321219174217826\n",
      "Epoch 3 iteration 0 loss 4.1373443603515625\n",
      "Epoch 3 iteration 100 loss 4.458973407745361\n",
      "Epoch 3 iteration 200 loss 4.416045188903809\n",
      "Epoch 3 iteration 300 loss 4.437078475952148\n",
      "Epoch 3 iteration 400 loss 4.151440143585205\n",
      "Epoch 3 iteration 500 loss 4.329726696014404\n",
      "Epoch 3 iteration 600 loss 4.318225860595703\n",
      "Epoch 3 iteration 700 loss 4.122394561767578\n",
      "Epoch 3 iteration 800 loss 4.369969367980957\n",
      "Epoch 3 iteration 900 loss 4.4184651374816895\n",
      "Epoch 3 iteration 1000 loss 4.420729637145996\n",
      "Epoch 3 iteration 1100 loss 4.580944538116455\n",
      "Epoch 3 iteration 1200 loss 3.841463327407837\n",
      "Epoch 3 iteration 1300 loss 4.487487316131592\n",
      "Epoch 3 iteration 1400 loss 4.2972517013549805\n",
      "Epoch 3 iteration 1500 loss 4.332420825958252\n",
      "Epoch 3 iteration 1600 loss 4.134940147399902\n",
      "Epoch 3 iteration 1700 loss 4.256643772125244\n",
      "Epoch 3 iteration 1800 loss 4.182973861694336\n",
      "Epoch 3 iteration 1900 loss 4.240206241607666\n",
      "Epoch 3 iteration 2000 loss 4.235872268676758\n",
      "Epoch 3 iteration 2100 loss 4.295577049255371\n",
      "Epoch 3 iteration 2200 loss 4.349503040313721\n",
      "Epoch 3 iteration 2300 loss 4.2983479499816895\n",
      "Epoch 3 iteration 2400 loss 4.123312950134277\n",
      "Epoch 3 iteration 2500 loss 4.273718357086182\n",
      "Epoch 3 iteration 2600 loss 4.214278697967529\n",
      "Epoch 3 iteration 2700 loss 4.218123912811279\n",
      "Epoch 3 iteration 2800 loss 4.161339282989502\n",
      "Epoch 3 iteration 2900 loss 4.1781086921691895\n",
      "Epoch 3 iteration 3000 loss 4.505728244781494\n",
      "Epoch 3 iteration 3100 loss 4.274317264556885\n",
      "Epoch 3 iteration 3200 loss 4.240739345550537\n",
      "Epoch 3 iteration 3300 loss 4.277515411376953\n",
      "Epoch 3 iteration 3400 loss 4.1983489990234375\n",
      "Epoch 3 iteration 3500 loss 4.1285552978515625\n",
      "Epoch 3 iteration 3600 loss 4.006635665893555\n",
      "Epoch 3 iteration 3700 loss 4.352885723114014\n",
      "Epoch 3 iteration 3800 loss 4.21630334854126\n",
      "Epoch 3 iteration 3900 loss 4.734842300415039\n",
      "Epoch 3 iteration 4000 loss 4.222026348114014\n",
      "Epoch 3 iteration 4100 loss 4.226931571960449\n",
      "Epoch 3 iteration 4200 loss 4.73878288269043\n",
      "Epoch 3 iteration 4300 loss 4.339847087860107\n",
      "Epoch 3 iteration 4400 loss 4.215916156768799\n",
      "Epoch 3 iteration 4500 loss 4.20546817779541\n",
      "Epoch 3 iteration 4600 loss 4.462703704833984\n",
      "Epoch 3 iteration 4700 loss 4.317254066467285\n",
      "Epoch 3 iteration 4800 loss 4.406203269958496\n",
      "Epoch 3 iteration 4900 loss 4.349771976470947\n",
      "Epoch 3 iteration 5000 loss 4.497073650360107\n",
      "Epoch 3 iteration 5100 loss 4.240240097045898\n",
      "Epoch 3 iteration 5200 loss 4.245280742645264\n",
      "Epoch 3 iteration 5300 loss 4.236097812652588\n",
      "Epoch 3 iteration 5400 loss 4.123876094818115\n",
      "Epoch 3 iteration 5500 loss 4.226821422576904\n",
      "Epoch 3 iteration 5600 loss 4.398310661315918\n",
      "Epoch 3 iteration 5700 loss 4.200277805328369\n",
      "Epoch 3 iteration 5800 loss 4.285842418670654\n",
      "Epoch 3 iteration 5900 loss 3.8995532989501953\n",
      "Epoch 3 iteration 6000 loss 4.329707145690918\n",
      "Epoch 3 iteration 6100 loss 4.226739406585693\n",
      "Epoch 3 iteration 6200 loss 4.2469868659973145\n",
      "Epoch 3 iteration 6300 loss 4.084589958190918\n",
      "Epoch 3 iteration 6400 loss 4.302097797393799\n",
      "Epoch 3 iteration 6500 loss 4.4600372314453125\n",
      "Epoch 3 iteration 6600 loss 4.265371322631836\n",
      "Epoch 3 iteration 6700 loss 4.304899215698242\n",
      "Epoch 3 iteration 6800 loss 4.22520112991333\n",
      "Epoch 3 iteration 6900 loss 4.311695098876953\n",
      "Epoch 3 iteration 7000 loss 4.260643482208252\n",
      "Epoch 3 iteration 7100 loss 4.312231540679932\n",
      "Epoch 3 iteration 7200 loss 4.373152256011963\n",
      "Epoch 3 iteration 7300 loss 4.2193284034729\n",
      "Epoch 3 iteration 7400 loss 4.137059211730957\n",
      "Epoch 3 iteration 7500 loss 4.116914749145508\n",
      "Epoch 3 iteration 7600 loss 4.460522174835205\n",
      "Epoch 3 iteration 7700 loss 4.220921993255615\n",
      "Epoch 3 iteration 7800 loss 4.307512283325195\n",
      "Epoch 3 iteration 7900 loss 4.058302402496338\n",
      "Epoch 3 iteration 8000 loss 4.049261093139648\n",
      "Epoch 3 iteration 8100 loss 4.4795613288879395\n",
      "Epoch 3 iteration 8200 loss 4.5157036781311035\n",
      "Epoch 3 iteration 8300 loss 4.442436695098877\n",
      "Epoch 3 iteration 8400 loss 4.2402753829956055\n",
      "Epoch 3 iteration 8500 loss 4.117795944213867\n",
      "Epoch 3 iteration 8600 loss 3.932549238204956\n",
      "Epoch 3 iteration 8700 loss 4.2910566329956055\n",
      "Epoch 3 iteration 8800 loss 4.339004039764404\n",
      "Epoch 3 iteration 8900 loss 4.317483425140381\n",
      "Epoch 3 iteration 9000 loss 4.223735332489014\n",
      "Epoch 3 iteration 9100 loss 4.337777137756348\n",
      "Epoch 3 iteration 9200 loss 4.352736949920654\n",
      "Epoch 3 iteration 9300 loss 4.291446208953857\n",
      "Epoch 3 iteration 9400 loss 4.10662317276001\n",
      "Epoch 3 iteration 9500 loss 4.29012393951416\n",
      "Epoch 3 iteration 9600 loss 4.275394439697266\n",
      "Epoch 3 iteration 9700 loss 4.345402717590332\n",
      "Epoch 3 iteration 9800 loss 4.317314624786377\n",
      "Epoch 3 Training loss 4.264167463939501\n",
      "Epoch 4 iteration 0 loss 4.094761848449707\n",
      "Epoch 4 iteration 100 loss 4.430906295776367\n",
      "Epoch 4 iteration 200 loss 4.339702129364014\n",
      "Epoch 4 iteration 300 loss 4.4353251457214355\n",
      "Epoch 4 iteration 400 loss 4.113397121429443\n",
      "Epoch 4 iteration 500 loss 4.257258892059326\n",
      "Epoch 4 iteration 600 loss 4.283435344696045\n",
      "Epoch 4 iteration 700 loss 4.070346832275391\n",
      "Epoch 4 iteration 800 loss 4.3280439376831055\n",
      "Epoch 4 iteration 900 loss 4.366650104522705\n",
      "Epoch 4 iteration 1000 loss 4.388364791870117\n",
      "Epoch 4 iteration 1100 loss 4.534362316131592\n",
      "Epoch 4 iteration 1200 loss 3.7849607467651367\n",
      "Epoch 4 iteration 1300 loss 4.382735252380371\n",
      "Epoch 4 iteration 1400 loss 4.283871650695801\n",
      "Epoch 4 iteration 1500 loss 4.28480339050293\n",
      "Epoch 4 iteration 1600 loss 4.1104817390441895\n",
      "Epoch 4 iteration 1700 loss 4.2110114097595215\n",
      "Epoch 4 iteration 1800 loss 4.126503944396973\n",
      "Epoch 4 iteration 1900 loss 4.193056583404541\n",
      "Epoch 4 iteration 2000 loss 4.204537391662598\n",
      "Epoch 4 iteration 2100 loss 4.249406814575195\n",
      "Epoch 4 iteration 2200 loss 4.370744228363037\n",
      "Epoch 4 iteration 2300 loss 4.289411544799805\n",
      "Epoch 4 iteration 2400 loss 4.1273193359375\n",
      "Epoch 4 iteration 2500 loss 4.209547996520996\n",
      "Epoch 4 iteration 2600 loss 4.1531662940979\n",
      "Epoch 4 iteration 2700 loss 4.171061038970947\n",
      "Epoch 4 iteration 2800 loss 4.120808124542236\n",
      "Epoch 4 iteration 2900 loss 4.100001811981201\n",
      "Epoch 4 iteration 3000 loss 4.456387519836426\n",
      "Epoch 4 iteration 3100 loss 4.225183010101318\n",
      "Epoch 4 iteration 3200 loss 4.187597274780273\n",
      "Epoch 4 iteration 3300 loss 4.212881088256836\n",
      "Epoch 4 iteration 3400 loss 4.207607746124268\n",
      "Epoch 4 iteration 3500 loss 4.11613655090332\n",
      "Epoch 4 iteration 3600 loss 3.976685047149658\n",
      "Epoch 4 iteration 3700 loss 4.31727933883667\n",
      "Epoch 4 iteration 3800 loss 4.182948112487793\n",
      "Epoch 4 iteration 3900 loss 4.694360256195068\n",
      "Epoch 4 iteration 4000 loss 4.159313678741455\n",
      "Epoch 4 iteration 4100 loss 4.2089338302612305\n",
      "Epoch 4 iteration 4200 loss 4.637477874755859\n",
      "Epoch 4 iteration 4300 loss 4.218120098114014\n",
      "Epoch 4 iteration 4400 loss 4.174178123474121\n",
      "Epoch 4 iteration 4500 loss 4.1190314292907715\n",
      "Epoch 4 iteration 4600 loss 4.415773391723633\n",
      "Epoch 4 iteration 4700 loss 4.350662708282471\n",
      "Epoch 4 iteration 4800 loss 4.373881816864014\n",
      "Epoch 4 iteration 4900 loss 4.340957164764404\n",
      "Epoch 4 iteration 5000 loss 4.4914655685424805\n",
      "Epoch 4 iteration 5100 loss 4.187495708465576\n",
      "Epoch 4 iteration 5200 loss 4.195706844329834\n",
      "Epoch 4 iteration 5300 loss 4.2059173583984375\n",
      "Epoch 4 iteration 5400 loss 4.125297546386719\n",
      "Epoch 4 iteration 5500 loss 4.166917324066162\n",
      "Epoch 4 iteration 5600 loss 4.369196891784668\n",
      "Epoch 4 iteration 5700 loss 4.148853302001953\n",
      "Epoch 4 iteration 5800 loss 4.2675862312316895\n",
      "Epoch 4 iteration 5900 loss 3.817117214202881\n",
      "Epoch 4 iteration 6000 loss 4.30664587020874\n",
      "Epoch 4 iteration 6100 loss 4.163249969482422\n",
      "Epoch 4 iteration 6200 loss 4.192113876342773\n",
      "Epoch 4 iteration 6300 loss 4.081295013427734\n",
      "Epoch 4 iteration 6400 loss 4.280977249145508\n",
      "Epoch 4 iteration 6500 loss 4.389852046966553\n",
      "Epoch 4 iteration 6600 loss 4.2243571281433105\n",
      "Epoch 4 iteration 6700 loss 4.26322603225708\n",
      "Epoch 4 iteration 6800 loss 4.177620887756348\n",
      "Epoch 4 iteration 6900 loss 4.276646137237549\n",
      "Epoch 4 iteration 7000 loss 4.196105003356934\n",
      "Epoch 4 iteration 7100 loss 4.358565330505371\n",
      "Epoch 4 iteration 7200 loss 4.3076910972595215\n",
      "Epoch 4 iteration 7300 loss 4.15131950378418\n",
      "Epoch 4 iteration 7400 loss 4.10686731338501\n",
      "Epoch 4 iteration 7500 loss 4.0785112380981445\n",
      "Epoch 4 iteration 7600 loss 4.438241481781006\n",
      "Epoch 4 iteration 7700 loss 4.2276387214660645\n",
      "Epoch 4 iteration 7800 loss 4.23237943649292\n",
      "Epoch 4 iteration 7900 loss 3.9825937747955322\n",
      "Epoch 4 iteration 8000 loss 4.025641918182373\n",
      "Epoch 4 iteration 8100 loss 4.424680709838867\n",
      "Epoch 4 iteration 8200 loss 4.449853897094727\n",
      "Epoch 4 iteration 8300 loss 4.436753273010254\n",
      "Epoch 4 iteration 8400 loss 4.262763500213623\n",
      "Epoch 4 iteration 8500 loss 4.119671821594238\n",
      "Epoch 4 iteration 8600 loss 3.9327690601348877\n",
      "Epoch 4 iteration 8700 loss 4.212386608123779\n",
      "Epoch 4 iteration 8800 loss 4.287766456604004\n",
      "Epoch 4 iteration 8900 loss 4.275090217590332\n",
      "Epoch 4 iteration 9000 loss 4.120099067687988\n",
      "Epoch 4 iteration 9100 loss 4.274074077606201\n",
      "Epoch 4 iteration 9200 loss 4.341525554656982\n",
      "Epoch 4 iteration 9300 loss 4.225788593292236\n",
      "Epoch 4 iteration 9400 loss 4.064235687255859\n",
      "Epoch 4 iteration 9500 loss 4.305633544921875\n",
      "Epoch 4 iteration 9600 loss 4.221609115600586\n",
      "Epoch 4 iteration 9700 loss 4.267580509185791\n",
      "Epoch 4 iteration 9800 loss 4.2469401359558105\n",
      "Epoch 4 Training loss 4.223005336419049\n",
      "Epoch 5 iteration 0 loss 4.0624284744262695\n",
      "Epoch 5 iteration 100 loss 4.389369964599609\n",
      "Epoch 5 iteration 200 loss 4.310202598571777\n",
      "Epoch 5 iteration 300 loss 4.374370098114014\n",
      "Epoch 5 iteration 400 loss 4.067865371704102\n",
      "Epoch 5 iteration 500 loss 4.239978313446045\n",
      "Epoch 5 iteration 600 loss 4.202436923980713\n",
      "Epoch 5 iteration 700 loss 4.033153057098389\n",
      "Epoch 5 iteration 800 loss 4.29418420791626\n",
      "Epoch 5 iteration 900 loss 4.375423431396484\n",
      "Epoch 5 iteration 1000 loss 4.350334167480469\n",
      "Epoch 5 iteration 1100 loss 4.529979228973389\n",
      "Epoch 5 iteration 1200 loss 3.7498600482940674\n",
      "Epoch 5 iteration 1300 loss 4.3606038093566895\n",
      "Epoch 5 iteration 1400 loss 4.238869667053223\n",
      "Epoch 5 iteration 1500 loss 4.236630916595459\n",
      "Epoch 5 iteration 1600 loss 4.095554351806641\n",
      "Epoch 5 iteration 1700 loss 4.217469215393066\n",
      "Epoch 5 iteration 1800 loss 4.062631130218506\n",
      "Epoch 5 iteration 1900 loss 4.184722900390625\n",
      "Epoch 5 iteration 2000 loss 4.181873321533203\n",
      "Epoch 5 iteration 2100 loss 4.195568084716797\n",
      "Epoch 5 iteration 2200 loss 4.3223114013671875\n",
      "Epoch 5 iteration 2300 loss 4.256076812744141\n",
      "Epoch 5 iteration 2400 loss 4.097878456115723\n",
      "Epoch 5 iteration 2500 loss 4.1787800788879395\n",
      "Epoch 5 iteration 2600 loss 4.107254505157471\n",
      "Epoch 5 iteration 2700 loss 4.134066104888916\n",
      "Epoch 5 iteration 2800 loss 4.04526424407959\n",
      "Epoch 5 iteration 2900 loss 4.098440647125244\n",
      "Epoch 5 iteration 3000 loss 4.444186687469482\n",
      "Epoch 5 iteration 3100 loss 4.205837726593018\n",
      "Epoch 5 iteration 3200 loss 4.112066268920898\n",
      "Epoch 5 iteration 3300 loss 4.203942775726318\n",
      "Epoch 5 iteration 3400 loss 4.1737380027771\n",
      "Epoch 5 iteration 3500 loss 4.040009498596191\n",
      "Epoch 5 iteration 3600 loss 3.901874303817749\n",
      "Epoch 5 iteration 3700 loss 4.258429527282715\n",
      "Epoch 5 iteration 3800 loss 4.152926921844482\n",
      "Epoch 5 iteration 3900 loss 4.61079216003418\n",
      "Epoch 5 iteration 4000 loss 4.163776874542236\n",
      "Epoch 5 iteration 4100 loss 4.12542200088501\n",
      "Epoch 5 iteration 4200 loss 4.597857475280762\n",
      "Epoch 5 iteration 4300 loss 4.218994617462158\n",
      "Epoch 5 iteration 4400 loss 4.178328514099121\n",
      "Epoch 5 iteration 4500 loss 4.0749688148498535\n",
      "Epoch 5 iteration 4600 loss 4.351924419403076\n",
      "Epoch 5 iteration 4700 loss 4.309974193572998\n",
      "Epoch 5 iteration 4800 loss 4.2840447425842285\n",
      "Epoch 5 iteration 4900 loss 4.274837017059326\n",
      "Epoch 5 iteration 5000 loss 4.411731719970703\n",
      "Epoch 5 iteration 5100 loss 4.146512985229492\n",
      "Epoch 5 iteration 5200 loss 4.186516284942627\n",
      "Epoch 5 iteration 5300 loss 4.1609907150268555\n",
      "Epoch 5 iteration 5400 loss 4.0841898918151855\n",
      "Epoch 5 iteration 5500 loss 4.1226582527160645\n",
      "Epoch 5 iteration 5600 loss 4.347054481506348\n",
      "Epoch 5 iteration 5700 loss 4.120939254760742\n",
      "Epoch 5 iteration 5800 loss 4.195618629455566\n",
      "Epoch 5 iteration 5900 loss 3.829087972640991\n",
      "Epoch 5 iteration 6000 loss 4.181276798248291\n",
      "Epoch 5 iteration 6100 loss 4.164214134216309\n",
      "Epoch 5 iteration 6200 loss 4.153556823730469\n",
      "Epoch 5 iteration 6300 loss 4.067331790924072\n",
      "Epoch 5 iteration 6400 loss 4.241043567657471\n",
      "Epoch 5 iteration 6500 loss 4.378899097442627\n",
      "Epoch 5 iteration 6600 loss 4.1822357177734375\n",
      "Epoch 5 iteration 6700 loss 4.23494291305542\n",
      "Epoch 5 iteration 6800 loss 4.136199951171875\n",
      "Epoch 5 iteration 6900 loss 4.220068454742432\n",
      "Epoch 5 iteration 7000 loss 4.160096645355225\n",
      "Epoch 5 iteration 7100 loss 4.3015851974487305\n",
      "Epoch 5 iteration 7200 loss 4.277584075927734\n",
      "Epoch 5 iteration 7300 loss 4.119543075561523\n",
      "Epoch 5 iteration 7400 loss 4.053688049316406\n",
      "Epoch 5 iteration 7500 loss 4.002266883850098\n",
      "Epoch 5 iteration 7600 loss 4.378396034240723\n",
      "Epoch 5 iteration 7700 loss 4.183043956756592\n",
      "Epoch 5 iteration 7800 loss 4.251765727996826\n",
      "Epoch 5 iteration 7900 loss 3.9864659309387207\n",
      "Epoch 5 iteration 8000 loss 3.979398727416992\n",
      "Epoch 5 iteration 8100 loss 4.413838863372803\n",
      "Epoch 5 iteration 8200 loss 4.388998985290527\n",
      "Epoch 5 iteration 8300 loss 4.401709079742432\n",
      "Epoch 5 iteration 8400 loss 4.175302505493164\n",
      "Epoch 5 iteration 8500 loss 4.080219268798828\n",
      "Epoch 5 iteration 8600 loss 3.887644052505493\n",
      "Epoch 5 iteration 8700 loss 4.211153030395508\n",
      "Epoch 5 iteration 8800 loss 4.2698283195495605\n",
      "Epoch 5 iteration 8900 loss 4.287430286407471\n",
      "Epoch 5 iteration 9000 loss 4.119315147399902\n",
      "Epoch 5 iteration 9100 loss 4.266959190368652\n",
      "Epoch 5 iteration 9200 loss 4.3575921058654785\n",
      "Epoch 5 iteration 9300 loss 4.2041215896606445\n",
      "Epoch 5 iteration 9400 loss 4.080079078674316\n",
      "Epoch 5 iteration 9500 loss 4.279469013214111\n",
      "Epoch 5 iteration 9600 loss 4.167484283447266\n",
      "Epoch 5 iteration 9700 loss 4.2704315185546875\n",
      "Epoch 5 iteration 9800 loss 4.206465244293213\n",
      "Epoch 5 Training loss 4.191186479570224\n",
      "Evaluation loss 4.225790490125334\n",
      "Epoch 6 iteration 0 loss 4.022202491760254\n",
      "Epoch 6 iteration 100 loss 4.3422770500183105\n",
      "Epoch 6 iteration 200 loss 4.251954078674316\n",
      "Epoch 6 iteration 300 loss 4.343289852142334\n",
      "Epoch 6 iteration 400 loss 4.055140495300293\n",
      "Epoch 6 iteration 500 loss 4.20616340637207\n",
      "Epoch 6 iteration 600 loss 4.235596179962158\n",
      "Epoch 6 iteration 700 loss 4.014815330505371\n",
      "Epoch 6 iteration 800 loss 4.2449493408203125\n",
      "Epoch 6 iteration 900 loss 4.282210826873779\n",
      "Epoch 6 iteration 1000 loss 4.357080936431885\n",
      "Epoch 6 iteration 1100 loss 4.490809917449951\n",
      "Epoch 6 iteration 1200 loss 3.7195003032684326\n",
      "Epoch 6 iteration 1300 loss 4.330821514129639\n",
      "Epoch 6 iteration 1400 loss 4.182713031768799\n",
      "Epoch 6 iteration 1500 loss 4.219784736633301\n",
      "Epoch 6 iteration 1600 loss 4.041718482971191\n",
      "Epoch 6 iteration 1700 loss 4.2207536697387695\n",
      "Epoch 6 iteration 1800 loss 4.0129523277282715\n",
      "Epoch 6 iteration 1900 loss 4.126839637756348\n",
      "Epoch 6 iteration 2000 loss 4.159095764160156\n",
      "Epoch 6 iteration 2100 loss 4.169835567474365\n",
      "Epoch 6 iteration 2200 loss 4.231587886810303\n",
      "Epoch 6 iteration 2300 loss 4.250950336456299\n",
      "Epoch 6 iteration 2400 loss 4.0615034103393555\n",
      "Epoch 6 iteration 2500 loss 4.158226490020752\n",
      "Epoch 6 iteration 2600 loss 4.060427665710449\n",
      "Epoch 6 iteration 2700 loss 4.120480537414551\n",
      "Epoch 6 iteration 2800 loss 4.005842685699463\n",
      "Epoch 6 iteration 2900 loss 4.089951515197754\n",
      "Epoch 6 iteration 3000 loss 4.371478080749512\n",
      "Epoch 6 iteration 3100 loss 4.171728610992432\n",
      "Epoch 6 iteration 3200 loss 4.0757317543029785\n",
      "Epoch 6 iteration 3300 loss 4.1821393966674805\n",
      "Epoch 6 iteration 3400 loss 4.100404262542725\n",
      "Epoch 6 iteration 3500 loss 4.015650272369385\n",
      "Epoch 6 iteration 3600 loss 3.876915454864502\n",
      "Epoch 6 iteration 3700 loss 4.381012439727783\n",
      "Epoch 6 iteration 3800 loss 4.158772945404053\n",
      "Epoch 6 iteration 3900 loss 4.627298831939697\n",
      "Epoch 6 iteration 4000 loss 4.135830879211426\n",
      "Epoch 6 iteration 4100 loss 4.117032051086426\n",
      "Epoch 6 iteration 4200 loss 4.581843376159668\n",
      "Epoch 6 iteration 4300 loss 4.194955348968506\n",
      "Epoch 6 iteration 4400 loss 4.148256301879883\n",
      "Epoch 6 iteration 4500 loss 4.105629920959473\n",
      "Epoch 6 iteration 4600 loss 4.360469341278076\n",
      "Epoch 6 iteration 4700 loss 4.282140731811523\n",
      "Epoch 6 iteration 4800 loss 4.304172992706299\n",
      "Epoch 6 iteration 4900 loss 4.238683700561523\n",
      "Epoch 6 iteration 5000 loss 4.38639497756958\n",
      "Epoch 6 iteration 5100 loss 4.125317573547363\n",
      "Epoch 6 iteration 5200 loss 4.144033432006836\n",
      "Epoch 6 iteration 5300 loss 4.132107257843018\n",
      "Epoch 6 iteration 5400 loss 4.076028823852539\n",
      "Epoch 6 iteration 5500 loss 4.127584934234619\n",
      "Epoch 6 iteration 5600 loss 4.312316417694092\n",
      "Epoch 6 iteration 5700 loss 4.141003131866455\n",
      "Epoch 6 iteration 5800 loss 4.164960861206055\n",
      "Epoch 6 iteration 5900 loss 3.8033792972564697\n",
      "Epoch 6 iteration 6000 loss 4.253355026245117\n",
      "Epoch 6 iteration 6100 loss 4.111536979675293\n",
      "Epoch 6 iteration 6200 loss 4.170202732086182\n",
      "Epoch 6 iteration 6300 loss 4.008697509765625\n",
      "Epoch 6 iteration 6400 loss 4.204448699951172\n",
      "Epoch 6 iteration 6500 loss 4.32029390335083\n",
      "Epoch 6 iteration 6600 loss 4.163560390472412\n",
      "Epoch 6 iteration 6700 loss 4.184716701507568\n",
      "Epoch 6 iteration 6800 loss 4.135841369628906\n",
      "Epoch 6 iteration 6900 loss 4.220365047454834\n",
      "Epoch 6 iteration 7000 loss 4.1937994956970215\n",
      "Epoch 6 iteration 7100 loss 4.317358493804932\n",
      "Epoch 6 iteration 7200 loss 4.237843036651611\n",
      "Epoch 6 iteration 7300 loss 4.076003074645996\n",
      "Epoch 6 iteration 7400 loss 4.01915168762207\n",
      "Epoch 6 iteration 7500 loss 3.9702658653259277\n",
      "Epoch 6 iteration 7600 loss 4.3844170570373535\n",
      "Epoch 6 iteration 7700 loss 4.139578342437744\n",
      "Epoch 6 iteration 7800 loss 4.225793361663818\n",
      "Epoch 6 iteration 7900 loss 3.9611964225769043\n",
      "Epoch 6 iteration 8000 loss 3.9503777027130127\n",
      "Epoch 6 iteration 8100 loss 4.393316268920898\n",
      "Epoch 6 iteration 8200 loss 4.398093223571777\n",
      "Epoch 6 iteration 8300 loss 4.377277374267578\n",
      "Epoch 6 iteration 8400 loss 4.1278791427612305\n",
      "Epoch 6 iteration 8500 loss 4.057892322540283\n",
      "Epoch 6 iteration 8600 loss 3.8366637229919434\n",
      "Epoch 6 iteration 8700 loss 4.186835765838623\n",
      "Epoch 6 iteration 8800 loss 4.1831464767456055\n",
      "Epoch 6 iteration 8900 loss 4.232813358306885\n",
      "Epoch 6 iteration 9000 loss 4.095926284790039\n",
      "Epoch 6 iteration 9100 loss 4.237479209899902\n",
      "Epoch 6 iteration 9200 loss 4.327991485595703\n",
      "Epoch 6 iteration 9300 loss 4.17896032333374\n",
      "Epoch 6 iteration 9400 loss 4.02712345123291\n",
      "Epoch 6 iteration 9500 loss 4.26064395904541\n",
      "Epoch 6 iteration 9600 loss 4.162372589111328\n",
      "Epoch 6 iteration 9700 loss 4.2709150314331055\n",
      "Epoch 6 iteration 9800 loss 4.251936912536621\n",
      "Epoch 6 Training loss 4.164852521360112\n",
      "Epoch 7 iteration 0 loss 4.020298004150391\n",
      "Epoch 7 iteration 100 loss 4.288553714752197\n",
      "Epoch 7 iteration 200 loss 4.23601770401001\n",
      "Epoch 7 iteration 300 loss 4.3335137367248535\n",
      "Epoch 7 iteration 400 loss 4.04526424407959\n",
      "Epoch 7 iteration 500 loss 4.177737236022949\n",
      "Epoch 7 iteration 600 loss 4.223209381103516\n",
      "Epoch 7 iteration 700 loss 4.037033557891846\n",
      "Epoch 7 iteration 800 loss 4.233975887298584\n",
      "Epoch 7 iteration 900 loss 4.31102180480957\n",
      "Epoch 7 iteration 1000 loss 4.260091781616211\n",
      "Epoch 7 iteration 1100 loss 4.488056182861328\n",
      "Epoch 7 iteration 1200 loss 3.719895362854004\n",
      "Epoch 7 iteration 1300 loss 4.301452159881592\n",
      "Epoch 7 iteration 1400 loss 4.199542999267578\n",
      "Epoch 7 iteration 1500 loss 4.174262046813965\n",
      "Epoch 7 iteration 1600 loss 4.049368858337402\n",
      "Epoch 7 iteration 1700 loss 4.122720241546631\n",
      "Epoch 7 iteration 1800 loss 3.9944944381713867\n",
      "Epoch 7 iteration 1900 loss 4.093682765960693\n",
      "Epoch 7 iteration 2000 loss 4.172318935394287\n",
      "Epoch 7 iteration 2100 loss 4.146052837371826\n",
      "Epoch 7 iteration 2200 loss 4.262627124786377\n",
      "Epoch 7 iteration 2300 loss 4.2106523513793945\n",
      "Epoch 7 iteration 2400 loss 4.0003132820129395\n",
      "Epoch 7 iteration 2500 loss 4.09962797164917\n",
      "Epoch 7 iteration 2600 loss 4.035085678100586\n",
      "Epoch 7 iteration 2700 loss 4.07332706451416\n",
      "Epoch 7 iteration 2800 loss 3.9429571628570557\n",
      "Epoch 7 iteration 2900 loss 4.049914836883545\n",
      "Epoch 7 iteration 3000 loss 4.32116174697876\n",
      "Epoch 7 iteration 3100 loss 4.149942874908447\n",
      "Epoch 7 iteration 3200 loss 4.10276985168457\n",
      "Epoch 7 iteration 3300 loss 4.134000301361084\n",
      "Epoch 7 iteration 3400 loss 4.0957417488098145\n",
      "Epoch 7 iteration 3500 loss 4.008126735687256\n",
      "Epoch 7 iteration 3600 loss 3.8589699268341064\n",
      "Epoch 7 iteration 3700 loss 4.208916187286377\n",
      "Epoch 7 iteration 3800 loss 4.157270908355713\n",
      "Epoch 7 iteration 3900 loss 4.60588264465332\n",
      "Epoch 7 iteration 4000 loss 4.158148765563965\n",
      "Epoch 7 iteration 4100 loss 4.101376056671143\n",
      "Epoch 7 iteration 4200 loss 4.581676006317139\n",
      "Epoch 7 iteration 4300 loss 4.142327785491943\n",
      "Epoch 7 iteration 4400 loss 4.105348587036133\n",
      "Epoch 7 iteration 4500 loss 4.082082271575928\n",
      "Epoch 7 iteration 4600 loss 4.332528114318848\n",
      "Epoch 7 iteration 4700 loss 4.226578235626221\n",
      "Epoch 7 iteration 4800 loss 4.299333572387695\n",
      "Epoch 7 iteration 4900 loss 4.24586296081543\n",
      "Epoch 7 iteration 5000 loss 4.398532867431641\n",
      "Epoch 7 iteration 5100 loss 4.113394260406494\n",
      "Epoch 7 iteration 5200 loss 4.121099948883057\n",
      "Epoch 7 iteration 5300 loss 4.123530387878418\n",
      "Epoch 7 iteration 5400 loss 4.039047718048096\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 64\u001B[0m\n\u001B[0;32m     61\u001B[0m             evaluate(model, dev_data)\n\u001B[0;32m     63\u001B[0m \u001B[38;5;66;03m# 训练\u001B[39;00m\n\u001B[1;32m---> 64\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[30], line 52\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, data, num_epochs)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# 更新\u001B[39;00m\n\u001B[0;32m     51\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 52\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m5.\u001B[39m)     \u001B[38;5;66;03m# 这里防止梯度爆炸， 这是和以往不太一样的地方\u001B[39;00m\n\u001B[0;32m     54\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mD:\\minconda\\envs\\class_env\\Lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\minconda\\envs\\class_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:282\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    273\u001B[0m inputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    274\u001B[0m     (inputs,)\n\u001B[0;32m    275\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, (torch\u001B[38;5;241m.\u001B[39mTensor, graph\u001B[38;5;241m.\u001B[39mGradientEdge))\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m()\n\u001B[0;32m    279\u001B[0m )\n\u001B[0;32m    281\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001B[38;5;28mlen\u001B[39m(tensors))\n\u001B[1;32m--> 282\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m \u001B[43m_make_grads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_grads_batched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n",
      "File \u001B[1;32mD:\\minconda\\envs\\class_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:161\u001B[0m, in \u001B[0;36m_make_grads\u001B[1;34m(outputs, grads, is_grads_batched)\u001B[0m\n\u001B[0;32m    155\u001B[0m         msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    156\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad can be implicitly created only for real scalar outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    157\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mout\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    158\u001B[0m         )\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg)\n\u001B[0;32m    160\u001B[0m     new_grads\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 161\u001B[0m         \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreserve_format\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    162\u001B[0m     )\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    164\u001B[0m     new_grads\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "dac5c48d-46eb-45b4-bed8-a318d127b4c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T08:22:23.415760Z",
     "start_time": "2024-12-03T08:22:23.387720Z"
    }
   },
   "source": "train_data[0]\n",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3468, 3907, 1936, ...,    0,    0,    0],\n",
       "        [7392, 6289,  667, ...,    0,    0,    0],\n",
       "        [7345, 5558, 1352, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 784, 5629, 3424, ..., 6542, 7493, 3218],\n",
       "        [ 393, 5868, 1338, ...,    0,    0,    0],\n",
       "        [5358, 2492, 3381, ...,    0,    0,    0]]),\n",
       " array([ 7, 10,  7,  7, 12, 17,  4, 13,  7, 13,  9, 13, 10, 12,  3, 12,  7,\n",
       "        12, 17,  7,  7, 15, 17,  7,  7, 12,  2,  7,  7, 22,  3, 12,  7, 18,\n",
       "         4,  7,  7,  7,  7,  7,  7,  7,  7, 12,  7,  7,  7, 11, 12,  2,  5,\n",
       "         7,  7,  7,  7,  5,  7,  7,  5,  7,  7, 27,  7, 13]),\n",
       " array([[ 398, 5353, 3814, ...,    0,    0,    0],\n",
       "        [5178, 8033, 6866, ...,    0,    0,    0],\n",
       "        [7067, 7224, 7308, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [2659, 3479, 7906, ..., 3332, 8250, 8127],\n",
       "        [7289, 7345, 1706, ...,    0,    0,    0],\n",
       "        [5345, 8053, 8250, ...,    0,    0,    0]]),\n",
       " array([ 7, 10,  7,  7, 12, 17,  4, 13,  7, 13,  9, 13, 10, 12,  3, 12,  7,\n",
       "        12, 17,  7,  7, 15, 17,  7,  7, 12,  2,  7,  7, 22,  3, 12,  7, 18,\n",
       "         4,  7,  7,  7,  7,  7,  7,  7,  7, 12,  7,  7,  7, 11, 12,  2,  5,\n",
       "         7,  7,  7,  7,  5,  7,  7,  5,  7,  7, 27,  7, 13]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d4621-32d8-43a6-8dec-f9aaf0dd7220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
