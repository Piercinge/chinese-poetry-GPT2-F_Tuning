{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载数据",
   "id": "dc1912676497aa1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:06:13.835101Z",
     "start_time": "2024-12-07T15:06:13.827482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from utils.file_utils import load_base_config\n",
    "\n",
    "# config = load_base_config(\"conf/config.yaml\")\n",
    "data_file = \"tang/tang.txt\""
   ],
   "id": "746c4b7e631fcf24",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:06:14.859743Z",
     "start_time": "2024-12-07T15:06:14.817718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取数据\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 去除空行和多余的空白符\n",
    "chinese_data = [line.strip() for line in lines if line.strip()]"
   ],
   "id": "8909afbfd5f2b2fe",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:06:16.465381Z",
     "start_time": "2024-12-07T15:06:16.460672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(chinese_data[:4])\n",
    "print(len(chinese_data))"
   ],
   "id": "c45316a2a37d44bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['度门能不访，冒雪屡西东。已想人如玉，遥怜马似骢。乍迷金谷路，稍变上阳宫。还比相思意，纷纷正满空。', '逍遥东城隅，双树寒葱蒨。广庭流华月，高阁凝余霰。杜门非养素，抱疾阻良䜩。孰谓无他人，思君岁云变。官曹亮先忝，陈躅慙俊彥。岂知晨与夜，相代不相见。缄书问所如，詶藻当芬绚。', '川上风雨来，须臾满城阙。岧峣青莲界，萧条孤兴发。前山遽已净，阴霭夜来歇。乔木生夏凉，流云吐华月。严城自有限，一水非难越。相望曙河远，高斋坐超忽。', '庭树忽已暗，故人那不来。祗因厌烦暑，永日坐霜台。']\n",
      "57586\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 文本切分，并用bert-base-chinese分词&存储",
   "id": "77266d4ece4d2591"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:06:24.976875Z",
     "start_time": "2024-12-07T15:06:19.366268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\", cache_dir=\"../model/bert-base-chinese\")\n",
    "tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '<|endoftext|>'})"
   ],
   "id": "794d78bee94c8037",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:36:58.701646Z",
     "start_time": "2024-12-06T16:35:44.230843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 添加结束标记 <|endoftext|> 到分词器\n",
    "tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '<|endoftext|>'})\n",
    "\n",
    "# 设置每个片段的最大 token 数\n",
    "context_length = 256\n",
    "\n",
    "def tokenize(doc):\n",
    "    # 为每个文档添加 <|endoftext|> 标记\n",
    "    text_with_eot = doc.replace(\"\\n\", \"\") + tokenizer.eos_token  # 去掉换行符  # 这里会使用已经添加的结束标记\n",
    "    tokens = tokenizer(\n",
    "        text_with_eot,\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_tensors=\"np\"\n",
    "    )[\"input_ids\"][0][1:-1]\n",
    "\n",
    "    tokens_np = np.array(tokens.flatten(), dtype=np.uint16)\n",
    "\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token 字典对于 uint16 来说太大\"\n",
    "    return tokens_np\n",
    "\n",
    "def write_datafile(filename, tokens_np):\n",
    "    np.save(filename, tokens_np)\n",
    "\n",
    "shard_size = int(1e6)  # 每个分片的 token 数\n",
    "\n",
    "output_dir = \"tang_tokenized_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 假设中文数据是一个列表，每个元素是一个文档文本\n",
    "chinese_data = chinese_data\n",
    "# chinese_data = [\"文档1的内容\", \"文档2的内容\", \"文档3的内容\"]  # 示例数据\n",
    "\n",
    "nprocs = max(1, os.cpu_count() // 2)\n",
    "# 标记所有文档并写入输出分片，每个分片shard_size令牌（最后一个分片有剩余）\n",
    "# with mp.Pool(nprocs) as pool: # 多线程\n",
    "shard_index = 0\n",
    "\n",
    "# preallocate buffer 以保存当前分片\n",
    "all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
    "token_count = 0\n",
    "progress_bar = None\n",
    "\n",
    "# 判断当前分片中是否有足够的空间用于新token？\n",
    "# for tokens in pool.imap(tokenize, chinese_data, chunksize=16):\n",
    "for token in chinese_data:\n",
    "    tokens = tokenize(token)\n",
    "    if token_count + len(tokens) < shard_size:\n",
    "        # 只需将 Token 附加到当前分片\n",
    "        all_tokens_np[token_count:token_count + len(tokens)] = tokens\n",
    "        token_count += len(tokens)\n",
    "\n",
    "        # 更新进度条\n",
    "        if progress_bar is None:\n",
    "            progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "        progress_bar.update(len(tokens))\n",
    "    else:\n",
    "        # 写入当前分片并启动新分片\n",
    "        split = \"val\" if shard_index == 0 else \"train\"\n",
    "        filename = os.path.join(output_dir, f\"tang_{split}_{shard_index:06d}.npy\")\n",
    "\n",
    "\n",
    "        # 将文档拆分为适合此分片的任何内容，其余的转到下一个\n",
    "        remainder = shard_size - token_count\n",
    "        progress_bar.update(remainder)\n",
    "        all_tokens_np[token_count:token_count + remainder] = tokens[:remainder]\n",
    "        write_datafile(filename, all_tokens_np)\n",
    "        shard_index += 1\n",
    "        progress_bar = None\n",
    "\n",
    "        # 使用当前文档的剩余部分填充下一个分片\n",
    "        all_tokens_np[0:len(tokens) - remainder] = tokens[remainder:]\n",
    "        token_count = len(tokens) - remainder\n",
    "\n",
    "# 将任何剩余的 Token 写入最后一个分片\n",
    "if token_count != 0:\n",
    "    split = \"val\" if shard_index == 0 else \"train\"\n",
    "    filename = os.path.join(output_dir, f\"tang_{split}_{shard_index:06d}.npy\")\n",
    "    write_datafile(filename, all_tokens_np[:token_count])"
   ],
   "id": "4ab79590555b962a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shard 0: 100%|██████████| 1000000/1000000 [00:42<00:00, 23536.61tokens/s]\n",
      "Shard 1:  13%|█▎        | 131206/1000000 [00:00<00:05, 153392.08tokens/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 50\u001B[0m\n\u001B[1;32m     47\u001B[0m tokens \u001B[38;5;241m=\u001B[39m tokenize(token)\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m token_count \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(tokens) \u001B[38;5;241m<\u001B[39m shard_size:\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;66;03m# 只需将 Token 附加到当前分片\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m     all_tokens_np[token_count:token_count \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m] \u001B[38;5;241m=\u001B[39m tokens\n\u001B[1;32m     51\u001B[0m     token_count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(tokens)\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;66;03m# 更新进度条\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:06:29.893401Z",
     "start_time": "2024-12-07T15:06:29.884466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 观察npy文件\n",
    "import numpy as np\n",
    "tokens = np.load(\"tang_tokenized_data/tang_train_000001.npy\") # 1000000\n",
    "print(len(tokens))"
   ],
   "id": "8f1da5224f26f03f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  测试bert-base-chinese分词器\n",
    "<br>\n",
    "\n",
    "1. 因为GPT是自回归的语言模型，不会停止输出，需要添加终止标志"
   ],
   "id": "979c548bd4040ef5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:18:43.347776Z",
     "start_time": "2024-12-07T15:18:43.270998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data = \"侯枉高鉴，举善掩瑕疵。斯民本已安，工拙两无施。何以酬明德，岁晏不磷缁。时节乃来集，欣怀方载驰。平明大府开，一得拜光辉。温如春风至，肃若严霜威。羣属所载瞻，而忘倦与饥。公堂燕华筵，礼罢复言辞。将从平门道，憩车沣水湄。山川降嘉。\"\n",
    "data1 = \"\"\"前年东秦夏六月，望日拜恩初赐玦。降秩削职迁黄州，仓皇束装三日发。故国东平不入城，北山一夜辞松栝。亲朋问讯若梦寐，骨肉分留作胡越。论罪岂合有民社，抵谳正欲加𫓧钺。仇家之议不尽用，天地寛仁日月察。才令分司置之蕲，上表谢恩秋已末。以御魑魅乃其分，欲居蛮夷圣犹屑。蕲也虽僻自善地，回环山溪富林樾。平生雅志在江湖，颇与蕲人相缔结。收拾孥属已团聚，南北无心更分别。白鱼煮玉秔炊珠，佐以秋菘与春蕨。筑室求田虽未就，典衣卖装略无阙。大谬不然心已忘，笑人非工已愈拙。人生端若梦栩栩，事去何庸书咄咄。大儿调邑换江南，要虽分房无远别。地劣两舍不宿舂，晨起为书午可达。邑四万户号难治，民杂江闽吏贪猾。锄彊洗恶勿著意，鱼逃至清人忌洁。化以诚心磨以久，教而后刑不怨杀。得闻无毁亦无誉，以慰萧萧双白髪。<|endoftext|>\n",
    "\"\"\"\n",
    "data = \"补吏多下迁，罢归聊自度。园庐既芜没，烟景空澹泊。闲居养疴瘵，守素甘葵藿。颜鬓日衰耗，冠带亦寥落。青苔已生路，绿筠始分箨。夕气下遥阴，微风动疎薄。草玄良见诮，杜门无请讬。非君好事者，谁来顾寂寞。\"\n",
    "# tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '<|endoftext|>'})\n",
    "print(1)\n",
    "outputs = tokenizer(\n",
    "    data + tokenizer.eos_token,\n",
    "    truncation=True,\n",
    "    # max_length=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n"
   ],
   "id": "c31e44b463dd1036",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:18:45.475461Z",
     "start_time": "2024-12-07T15:18:45.466462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 可以发现实际上对诗词进行tokenization时，token数和文本数基本是等长的，编码 21128 是 <|endoftext|>\n",
    "# print(outputs)\n",
    "print(outputs[\"input_ids\"][0][1:-1])\n",
    "print(len(outputs[\"input_ids\"][0][1:-1]))\n",
    "print(len(data)+1)  # <|endoftext|>: 1\n",
    "# print(outputs[\"attention_mask\"])\n",
    "# print(outputs[\"token_type_ids\"])"
   ],
   "id": "79ddecd7f8beebb9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6133, 1401, 1914, 678, 6810, 8024, 5387, 2495, 5464, 5632, 2428, 511, 1736, 2416, 3188, 5697, 3766, 8024, 4170, 3250, 4958, 4079, 3788, 511, 7312, 2233, 1075, 100, 100, 8024, 2127, 5162, 4491, 5878, 5977, 511, 7582, 7779, 3189, 6139, 5450, 8024, 1094, 2372, 771, 2178, 5862, 511, 7471, 5726, 2347, 4495, 6662, 8024, 5344, 5035, 1993, 1146, 100, 511, 1911, 3698, 678, 6898, 7346, 8024, 2544, 7599, 1220, 100, 5946, 511, 5770, 4371, 5679, 6224, 100, 8024, 3336, 7305, 3187, 6435, 100, 511, 7478, 1409, 1962, 752, 5442, 8024, 6443, 3341, 7560, 2163, 2174, 511, 21128]\n",
      "97\n",
      "97\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:13:52.505148Z",
     "start_time": "2024-12-06T11:13:52.496541Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6133, 1401, 1914, 678, 6810, 8024, 5387, 2495, 5464, 5632, 2428, 511, 1736, 2416, 3188, 5697, 3766, 8024, 4170, 3250, 4958, 4079, 3788, 511, 7312, 2233, 1075, 100, 100, 8024, 2127, 5162, 4491, 5878, 5977, 511, 7582, 7779, 3189, 6139, 5450, 8024, 1094, 2372, 771, 2178, 5862, 511, 7471, 5726, 2347, 4495, 6662, 8024, 5344, 5035, 1993, 1146, 100, 511, 1911, 3698, 678, 6898, 7346, 8024, 2544, 7599, 1220, 100, 5946, 511, 5770, 4371, 5679, 6224, 100, 8024, 3336, 7305, 3187, 6435, 100, 511, 7478, 1409, 1962, 752, 5442, 8024, 6443, 3341, 7560, 2163, 2174, 511, 21128]\n",
      "97\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "# 可以发现实际上对诗词进行tokenization时，token数和文本数基本是等长的，编码 21128 是 <|endoftext|>\n",
    "# print(outputs)\n",
    "print(outputs[\"input_ids\"][0][1:-1])\n",
    "print(len(data)+1)  # <|endoftext|>: 1\n",
    "# print(outputs[\"attention_mask\"])\n",
    "# print(outputs[\"token_type_ids\"])"
   ],
   "id": "58eab1d995359d18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 解码测试\n",
    "text = tokenizer.decode(outputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(outputs[\"input_ids\"][0])\n",
    "# 打印解码后的文本\n",
    "print(text)"
   ],
   "id": "ab1204fd9492a449",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.encode(\"谁 来 顾 寂 寞\")",
   "id": "9eb83f65ce9838b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test = tokenizer(\n",
    "        data,\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_tensors=\"np\"\n",
    "    )[\"input_ids\"]\n",
    "test"
   ],
   "id": "24ab950153d8cf40",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
