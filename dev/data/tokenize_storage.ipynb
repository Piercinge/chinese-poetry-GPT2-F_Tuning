{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载数据",
   "id": "dc1912676497aa1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T05:31:59.081773Z",
     "start_time": "2024-12-06T05:31:58.983087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.file_utils import load_base_config\n",
    "\n",
    "config = load_base_config(\"conf/config.yaml\")\n",
    "data_file = \"tang/tang.txt\""
   ],
   "id": "746c4b7e631fcf24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\B_pythonProject\\chinese-poetry-BERT-F_Tuning\\conf/config.yaml\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T05:32:20.213718Z",
     "start_time": "2024-12-06T05:32:20.119539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取数据\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 去除空行和多余的空白符\n",
    "chinese_data = [line.strip() for line in lines if line.strip()]"
   ],
   "id": "8909afbfd5f2b2fe",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T05:32:21.688500Z",
     "start_time": "2024-12-06T05:32:21.683490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(chinese_data[:4])\n",
    "print(len(chinese_data))"
   ],
   "id": "c45316a2a37d44bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['度门能不访，冒雪屡西东。已想人如玉，遥怜马似骢。乍迷金谷路，稍变上阳宫。还比相思意，纷纷正满空。', '逍遥东城隅，双树寒葱蒨。广庭流华月，高阁凝余霰。杜门非养素，抱疾阻良䜩。孰谓无他人，思君岁云变。官曹亮先忝，陈躅慙俊彥。岂知晨与夜，相代不相见。缄书问所如，詶藻当芬绚。', '川上风雨来，须臾满城阙。岧峣青莲界，萧条孤兴发。前山遽已净，阴霭夜来歇。乔木生夏凉，流云吐华月。严城自有限，一水非难越。相望曙河远，高斋坐超忽。', '庭树忽已暗，故人那不来。祗因厌烦暑，永日坐霜台。']\n",
      "57580\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 文本切分，并用bert-base-chinese分词&存储",
   "id": "77266d4ece4d2591"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T05:32:32.082850Z",
     "start_time": "2024-12-06T05:32:24.331073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\", cache_dir=\"../model/bert-base-chinese\")\n",
    "tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '<|endoftext|>'})"
   ],
   "id": "794d78bee94c8037",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T05:33:09.990980Z",
     "start_time": "2024-12-06T05:33:01.506782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 添加结束标记 <|endoftext|> 到分词器\n",
    "tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '<|endoftext|>'})\n",
    "\n",
    "# 设置每个片段的最大 token 数\n",
    "context_length = 128\n",
    "\n",
    "def tokenize(doc):\n",
    "    # 为每个文档添加 <|endoftext|> 标记\n",
    "    text_with_eot = doc + tokenizer.eos_token  # 这里会使用已经添加的结束标记\n",
    "    tokens = tokenizer(\n",
    "        text_with_eot,\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_tensors=\"np\"\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    tokens_np = np.array(tokens.flatten(), dtype=np.uint16)\n",
    "\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token 字典对于 uint16 来说太大\"\n",
    "    return tokens_np\n",
    "\n",
    "def write_datafile(filename, tokens_np):\n",
    "    np.save(filename, tokens_np)\n",
    "\n",
    "shard_size = int(1e6)  # 每个分片的 token 数\n",
    "\n",
    "output_dir = \"tang_tokenized_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 假设中文数据是一个列表，每个元素是一个文档文本\n",
    "chinese_data = chinese_data\n",
    "# chinese_data = [\"文档1的内容\", \"文档2的内容\", \"文档3的内容\"]  # 示例数据\n",
    "\n",
    "nprocs = max(1, os.cpu_count() // 2)\n",
    "# 标记所有文档并写入输出分片，每个分片shard_size令牌（最后一个分片有剩余）\n",
    "# with mp.Pool(nprocs) as pool: # 多线程\n",
    "shard_index = 0\n",
    "\n",
    "# preallocate buffer 以保存当前分片\n",
    "all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
    "token_count = 0\n",
    "progress_bar = None\n",
    "\n",
    "# 判断当前分片中是否有足够的空间用于新token？\n",
    "# for tokens in pool.imap(tokenize, chinese_data, chunksize=16):\n",
    "for token in chinese_data:\n",
    "    tokens = tokenize(token)\n",
    "    if token_count + len(tokens) < shard_size:\n",
    "        # 只需将 Token 附加到当前分片\n",
    "        all_tokens_np[token_count:token_count + len(tokens)] = tokens\n",
    "        token_count += len(tokens)\n",
    "\n",
    "        # 更新进度条\n",
    "        if progress_bar is None:\n",
    "            progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "        progress_bar.update(len(tokens))\n",
    "    else:\n",
    "        # 写入当前分片并启动新分片\n",
    "        split = \"val\" if shard_index == 0 else \"train\"\n",
    "        filename = os.path.join(output_dir, f\"tang_{split}_{shard_index:06d}.npy\")\n",
    "\n",
    "        # 将文档拆分为适合此分片的任何内容，其余的转到下一个\n",
    "        remainder = shard_size - token_count\n",
    "        progress_bar.update(remainder)\n",
    "        all_tokens_np[token_count:token_count + remainder] = tokens[:remainder]\n",
    "        write_datafile(filename, all_tokens_np)\n",
    "        shard_index += 1\n",
    "        progress_bar = None\n",
    "\n",
    "        # 使用当前文档的剩余部分填充下一个分片\n",
    "        all_tokens_np[0:len(tokens) - remainder] = tokens[remainder:]\n",
    "        token_count = len(tokens) - remainder\n",
    "\n",
    "# 将任何剩余的 Token 写入最后一个分片\n",
    "if token_count != 0:\n",
    "    split = \"val\" if shard_index == 0 else \"train\"\n",
    "    filename = os.path.join(output_dir, f\"tang_{split}_{shard_index:06d}.npy\")\n",
    "    write_datafile(filename, all_tokens_np[:token_count])"
   ],
   "id": "4ab79590555b962a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shard 0: 100%|██████████| 1000000/1000000 [00:02<00:00, 360151.79tokens/s]\n",
      "Shard 1: 100%|█████████▉| 999926/1000000 [00:02<00:00, 374234.10tokens/s]\n",
      "Shard 2: 100%|█████████▉| 999977/1000000 [00:02<00:00, 390991.23tokens/s]\n",
      "Shard 3:  17%|█▋        | 172270/1000000 [00:00<00:01, 435778.90tokens/s]"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 观察npy文件\n",
    "import numpy as np\n",
    "tokens = np.load(\"tang_tokenized_data/tang_shard_000003.npy\") # 1000000\n",
    "print(len(tokens))"
   ],
   "id": "8f1da5224f26f03f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T03:22:53.383252Z",
     "start_time": "2024-12-06T03:22:53.378489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 观察npy文件\n",
    "import numpy as np\n",
    "tokens = np.load(\"tang_tokenized_data/tang_shard_000003.npy\") # 1000000\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ],
   "id": "b48d46179bc89a09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193395\n",
      "[  511  1426  2273 ...   511 21128   102]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  测试bert-base-chinese分词器\n",
    "<br>\n",
    "\n",
    "1. 因为GPT是自回归的语言模型，不会停止输出，需要添加终止标志"
   ],
   "id": "979c548bd4040ef5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T03:25:00.829736Z",
     "start_time": "2024-12-06T03:25:00.824450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data = \"侯枉高鉴，举善掩瑕疵。斯民本已安，工拙两无施。何以酬明德，岁晏不磷缁。时节乃来集，欣怀方载驰。平明大府开，一得拜光辉。温如春风至，肃若严霜威。羣属所载瞻，而忘倦与饥。公堂燕华筵，礼罢复言辞。将从平门道，憩车沣水湄。山川降嘉。\"\n",
    "data = \"补吏多下迁，罢归聊自度。园庐既芜没，烟景空澹泊。闲居养疴瘵，守素甘葵藿。颜鬓日衰耗，冠带亦寥落。青苔已生路，绿筠始分箨。夕气下遥阴，微风动疎薄。草玄良见诮，杜门无请讬。非君好事者，谁来顾寂寞。\"\n",
    "# tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '<|endoftext|>'})\n",
    "print(1)\n",
    "outputs = tokenizer(\n",
    "    data + tokenizer.eos_token,\n",
    "    truncation=True,\n",
    "    max_length=125,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n"
   ],
   "id": "c31e44b463dd1036",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T03:25:02.494685Z",
     "start_time": "2024-12-06T03:25:02.491124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 可以发现实际上对诗词进行tokenization时，token数和文本数基本是等长的，编码 21128 是 <|endoftext|>\n",
    "# print(outputs)\n",
    "print(outputs[\"input_ids\"][0])\n",
    "print(len(data)+1)  # <|endoftext|>: 1\n",
    "# print(outputs[\"attention_mask\"])\n",
    "# print(outputs[\"token_type_ids\"])"
   ],
   "id": "79ddecd7f8beebb9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6133, 1401, 1914, 678, 6810, 8024, 5387, 2495, 5464, 5632, 2428, 511, 1736, 2416, 3188, 5697, 3766, 8024, 4170, 3250, 4958, 4079, 3788, 511, 7312, 2233, 1075, 100, 100, 8024, 2127, 5162, 4491, 5878, 5977, 511, 7582, 7779, 3189, 6139, 5450, 8024, 1094, 2372, 771, 2178, 5862, 511, 7471, 5726, 2347, 4495, 6662, 8024, 5344, 5035, 1993, 1146, 100, 511, 1911, 3698, 678, 6898, 7346, 8024, 2544, 7599, 1220, 100, 5946, 511, 5770, 4371, 5679, 6224, 100, 8024, 3336, 7305, 3187, 6435, 100, 511, 7478, 1409, 1962, 752, 5442, 8024, 6443, 3341, 7560, 2163, 2174, 511, 21128, 102]\n",
      "97\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T03:26:06.126229Z",
     "start_time": "2024-12-06T03:26:06.119074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 解码测试\n",
    "text = tokenizer.decode(outputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(outputs[\"input_ids\"][0])\n",
    "# 打印解码后的文本\n",
    "print(text)"
   ],
   "id": "ab1204fd9492a449",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6133, 1401, 1914, 678, 6810, 8024, 5387, 2495, 5464, 5632, 2428, 511, 1736, 2416, 3188, 5697, 3766, 8024, 4170, 3250, 4958, 4079, 3788, 511, 7312, 2233, 1075, 100, 100, 8024, 2127, 5162, 4491, 5878, 5977, 511, 7582, 7779, 3189, 6139, 5450, 8024, 1094, 2372, 771, 2178, 5862, 511, 7471, 5726, 2347, 4495, 6662, 8024, 5344, 5035, 1993, 1146, 100, 511, 1911, 3698, 678, 6898, 7346, 8024, 2544, 7599, 1220, 100, 5946, 511, 5770, 4371, 5679, 6224, 100, 8024, 3336, 7305, 3187, 6435, 100, 511, 7478, 1409, 1962, 752, 5442, 8024, 6443, 3341, 7560, 2163, 2174, 511, 21128, 102]\n",
      "补 吏 多 下 迁 ， 罢 归 聊 自 度 。 园 庐 既 芜 没 ， 烟 景 空 澹 泊 。 闲 居 养 ， 守 素 甘 葵 藿 。 颜 鬓 日 衰 耗 ， 冠 带 亦 寥 落 。 青 苔 已 生 路 ， 绿 筠 始 分 。 夕 气 下 遥 阴 ， 微 风 动 薄 。 草 玄 良 见 ， 杜 门 无 请 。 非 君 好 事 者 ， 谁 来 顾 寂 寞 。\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T03:22:06.357971Z",
     "start_time": "2024-12-06T03:22:06.350312Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.encode(\"谁 来 顾 寂 寞\")",
   "id": "9eb83f65ce9838b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 6443, 3341, 7560, 2163, 2174, 102]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T14:11:06.453282Z",
     "start_time": "2024-12-05T14:11:06.447678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test = tokenizer(\n",
    "        data,\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_tensors=\"np\"\n",
    "    )[\"input_ids\"]\n",
    "test"
   ],
   "id": "24ab950153d8cf40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 6133, 1401, 1914,  678, 6810, 8024, 5387, 2495, 5464, 5632,\n",
       "        2428,  511, 1736, 2416, 3188, 5697, 3766, 8024, 4170, 3250, 4958,\n",
       "        4079, 3788,  511, 7312, 2233, 1075,  100,  100, 8024, 2127, 5162,\n",
       "        4491, 5878, 5977,  511, 7582, 7779, 3189, 6139, 5450, 8024, 1094,\n",
       "        2372,  771, 2178, 5862,  511, 7471, 5726, 2347, 4495, 6662, 8024,\n",
       "        5344, 5035, 1993, 1146,  100,  511, 1911, 3698,  678, 6898, 7346,\n",
       "        8024, 2544, 7599, 1220,  100, 5946,  511, 5770, 4371, 5679, 6224,\n",
       "         100, 8024, 3336, 7305, 3187, 6435,  100,  511, 7478, 1409, 1962,\n",
       "         752, 5442, 8024, 6443, 3341, 7560, 2163, 2174,  511,  102]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
