{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载数据",
   "id": "dc1912676497aa1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:43:22.936316Z",
     "start_time": "2024-12-05T11:43:22.930753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.file_utils import load_base_config\n",
    "\n",
    "config = load_base_config(\"conf/config.yaml\")\n",
    "data_file = \"../tang/tang.txt\""
   ],
   "id": "cd05ec7d398792d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\B_pythonProject\\chinese-poetry-BERT-F_Tuning\\conf/config.yaml\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T11:43:26.599881Z",
     "start_time": "2024-12-05T11:43:26.566290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取数据\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 去除空行和多余的空白符\n",
    "chinese_data = [line.strip() for line in lines if line.strip()]"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:43:28.318290Z",
     "start_time": "2024-12-05T11:43:28.314138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(chinese_data[:4])\n",
    "print(len(chinese_data))"
   ],
   "id": "c45316a2a37d44bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['度门能不访，冒雪屡西东。已想人如玉，遥怜马似骢。乍迷金谷路，稍变上阳宫。还比相思意，纷纷正满空。', '逍遥东城隅，双树寒葱蒨。广庭流华月，高阁凝余霰。杜门非养素，抱疾阻良䜩。孰谓无他人，思君岁云变。官曹亮先忝，陈躅慙俊彥。岂知晨与夜，相代不相见。缄书问所如，詶藻当芬绚。', '川上风雨来，须臾满城阙。岧峣青莲界，萧条孤兴发。前山遽已净，阴霭夜来歇。乔木生夏凉，流云吐华月。严城自有限，一水非难越。相望曙河远，高斋坐超忽。', '庭树忽已暗，故人那不来。祗因厌烦暑，永日坐霜台。']\n",
      "57580\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 文本切分，并用bert-base-chinese分词&存储",
   "id": "77266d4ece4d2591"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:39:08.286202Z",
     "start_time": "2024-12-05T11:38:58.915245Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shard 0: 100%|██████████| 1000000/1000000 [00:02<00:00, 399612.19tokens/s]\n",
      "Shard 1: 100%|█████████▉| 999926/1000000 [00:02<00:00, 417474.89tokens/s]\n",
      "Shard 2: 100%|█████████▉| 999977/1000000 [00:02<00:00, 402668.73tokens/s]\n",
      "Shard 3:  18%|█▊        | 180256/1000000 [00:00<00:01, 456972.38tokens/s]"
     ]
    }
   ],
   "execution_count": 9,
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 添加结束标记 <|endoftext|> 到分词器\n",
    "tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '<|endoftext|>'})\n",
    "\n",
    "# 设置每个片段的最大 token 数\n",
    "context_length = 512\n",
    "\n",
    "def tokenize(doc):\n",
    "    # 为每个文档添加 <|endoftext|> 标记\n",
    "    text_with_eot = doc + tokenizer.eos_token  # 这里会使用已经添加的结束标记\n",
    "    tokens = tokenizer(\n",
    "        text_with_eot,\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_tensors=\"np\"\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    tokens_np = np.array(tokens.flatten(), dtype=np.uint16)\n",
    "\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token 字典对于 uint16 来说太大\"\n",
    "    return tokens_np\n",
    "\n",
    "def write_datafile(filename, tokens_np):\n",
    "    np.save(filename, tokens_np)\n",
    "\n",
    "shard_size = int(1e6)  # 每个分片的 token 数\n",
    "\n",
    "output_dir = \"bert_tokenized_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 假设中文数据是一个列表，每个元素是一个文档文本\n",
    "chinese_data = chinese_data\n",
    "# chinese_data = [\"文档1的内容\", \"文档2的内容\", \"文档3的内容\"]  # 示例数据\n",
    "\n",
    "nprocs = max(1, os.cpu_count() // 2)\n",
    "# 标记所有文档并写入输出分片，每个分片shard_size令牌（最后一个分片有剩余）\n",
    "# with mp.Pool(nprocs) as pool: # 多线程\n",
    "shard_index = 0\n",
    "\n",
    "# preallocate buffer 以保存当前分片\n",
    "all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
    "token_count = 0\n",
    "progress_bar = None\n",
    "\n",
    "# 判断当前分片中是否有足够的空间用于新token？\n",
    "# for tokens in pool.imap(tokenize, chinese_data, chunksize=16):\n",
    "for token in chinese_data:\n",
    "    tokens = tokenize(token)\n",
    "    if token_count + len(tokens) < shard_size:\n",
    "        # 只需将 Token 附加到当前分片\n",
    "        all_tokens_np[token_count:token_count + len(tokens)] = tokens\n",
    "        token_count += len(tokens)\n",
    "\n",
    "        # 更新进度条\n",
    "        if progress_bar is None:\n",
    "            progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "        progress_bar.update(len(tokens))\n",
    "    else:\n",
    "        # 写入当前分片并启动新分片\n",
    "        filename = os.path.join(output_dir, f\"bert_shard_{shard_index:06d}.npy\")\n",
    "\n",
    "        # 将文档拆分为适合此分片的任何内容，其余的转到下一个\n",
    "        remainder = shard_size - token_count\n",
    "        progress_bar.update(remainder)\n",
    "        all_tokens_np[token_count:token_count + remainder] = tokens[:remainder]\n",
    "        write_datafile(filename, all_tokens_np)\n",
    "        shard_index += 1\n",
    "        progress_bar = None\n",
    "\n",
    "        # 使用当前文档的剩余部分填充下一个分片\n",
    "        all_tokens_np[0:len(tokens) - remainder] = tokens[remainder:]\n",
    "        token_count = len(tokens) - remainder\n",
    "\n",
    "# 将任何剩余的 Token 写入最后一个分片\n",
    "if token_count != 0:\n",
    "    filename = os.path.join(output_dir, f\"bert_shard_{shard_index:06d}.npy\")\n",
    "    write_datafile(filename, all_tokens_np[:token_count])"
   ],
   "id": "3299afe64f75acdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:48:00.918754Z",
     "start_time": "2024-12-05T11:48:00.912306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 观察npy文件\n",
    "import numpy as np\n",
    "tokens = np.load(\"tang_tokenized_data/bert_shard_000000.npy\")\n",
    "print(tokens)"
   ],
   "id": "a9f7838b3738d2df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 101 2428 7305 ... 6037  511  102]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  测试bert-base-chinese分词器\n",
    "<br>\n",
    "\n",
    "1. 因为GPT是自回归的语言模型，不会停止输出，需要添加终止标志"
   ],
   "id": "24e92971f3ae80c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:48:57.439168Z",
     "start_time": "2024-12-05T11:48:57.434988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data = \"侯枉高鉴，举善掩瑕疵。斯民本已安，工拙两无施。何以酬明德，岁晏不磷缁。时节乃来集，欣怀方载驰。平明大府开，一得拜光辉。温如春风至，肃若严霜威。羣属所载瞻，而忘倦与饥。公堂燕华筵，礼罢复言辞。将从平门道，憩车沣水湄。山川降嘉。\"\n",
    "data = \"补吏多下迁，罢归聊自度。园庐既芜没，烟景空澹泊。闲居养疴瘵，守素甘葵藿。颜鬓日衰耗，冠带亦寥落。青苔已生路，绿筠始分箨。夕气下遥阴，微风动疎薄。草玄良见诮，杜门无请讬。非君好事者，谁来顾寂寞。\"\n",
    "tokenizer.add_special_tokens(special_tokens_dict={'eos_token': '<|endoftext|>'})\n",
    "\n",
    "outputs = tokenizer(\n",
    "    data + tokenizer.eos_token,\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n"
   ],
   "id": "1b46f7227df1d9e7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:48:58.878319Z",
     "start_time": "2024-12-05T11:48:58.874474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 可以发现实际上对诗词进行tokenization时，token数和文本数基本是等长的，编码 21128 是 <|endoftext|>\n",
    "# print(outputs)\n",
    "print(len(outputs[\"input_ids\"][0]))\n",
    "print(len(data)+1)  # <|endoftext|>: 1\n",
    "# print(outputs[\"attention_mask\"])\n",
    "# print(outputs[\"token_type_ids\"])"
   ],
   "id": "5a66a2d8573c1204",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "97\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:49:00.273074Z",
     "start_time": "2024-12-05T11:49:00.265811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 解码测试\n",
    "text = tokenizer.decode(outputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(outputs[\"input_ids\"][0])\n",
    "# 打印解码后的文本\n",
    "print(text)"
   ],
   "id": "5dbf8febc73682e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6133, 1401, 1914, 678, 6810, 8024, 5387, 2495, 5464, 5632, 2428, 511, 1736, 2416, 3188, 5697, 3766, 8024, 4170, 3250, 4958, 4079, 3788, 511, 7312, 2233, 1075, 100, 100, 8024, 2127, 5162, 4491, 5878, 5977, 511, 7582, 7779, 3189, 6139, 5450, 8024, 1094, 2372, 771, 2178, 5862, 511, 7471, 5726, 2347, 4495, 6662, 8024, 5344, 5035, 1993, 1146, 100, 511, 1911, 3698, 678, 6898, 7346, 8024, 2544, 7599, 1220, 100, 5946, 511, 5770, 4371, 5679, 6224, 100, 8024, 3336, 7305, 3187, 6435, 100, 511, 7478, 1409, 1962, 752, 5442, 8024, 6443, 3341, 7560, 2163, 2174, 511, 21128, 102]\n",
      "补 吏 多 下 迁 ， 罢 归 聊 自 度 。 园 庐 既 芜 没 ， 烟 景 空 澹 泊 。 闲 居 养 ， 守 素 甘 葵 藿 。 颜 鬓 日 衰 耗 ， 冠 带 亦 寥 落 。 青 苔 已 生 路 ， 绿 筠 始 分 。 夕 气 下 遥 阴 ， 微 风 动 薄 。 草 玄 良 见 ， 杜 门 无 请 。 非 君 好 事 者 ， 谁 来 顾 寂 寞 。\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
