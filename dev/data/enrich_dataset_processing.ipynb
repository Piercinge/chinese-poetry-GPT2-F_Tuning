{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-07T07:09:36.456462Z",
     "start_time": "2024-12-07T07:09:36.447336Z"
    }
   },
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "# 示例文本\n",
    "text = \"请告严程尽，西归道路寒。欲陪鹰隼集，犹恋鹡鸰单。洛邑人全少，嵩高雪尚残。满台谁不故，报我在微官。\"\n",
    "\n",
    "# 使用 Jieba 分词\n",
    "words = jieba.lcut(text)\n",
    "\n",
    "# 统计词频\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# 构建词表\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "# 定义编码函数\n",
    "def encode(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return [word_to_index[word] for word in words if word in word_to_index]\n",
    "\n",
    "# 定义解码函数\n",
    "def decode(indices):\n",
    "    return ' '.join(index_to_word[idx] for idx in indices if idx in index_to_word)\n",
    "\n",
    "# 示例编码和解码\n",
    "encoded_text = encode(text)\n",
    "decoded_text = decode(encoded_text)\n",
    "\n",
    "print(\"原始文本:\", text)\n",
    "print(\"编码结果:\", encoded_text)\n",
    "print(\"解码结果:\", decoded_text)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本: 请告严程尽，西归道路寒。欲陪鹰隼集，犹恋鹡鸰单。洛邑人全少，嵩高雪尚残。满台谁不故，报我在微官。\n",
      "编码结果: [2, 3, 4, 0, 5, 6, 7, 1, 8, 9, 10, 11, 0, 12, 13, 14, 15, 1, 16, 17, 18, 0, 19, 20, 1, 21, 22, 23, 24, 0, 25, 26, 27, 28, 1]\n",
      "解码结果: 请告 严程 尽 ， 西归 道路 寒 。 欲 陪 鹰隼 集 ， 犹恋 鹡 鸰 单 。 洛邑 人 全少 ， 嵩高雪尚 残 。 满台 谁 不 故 ， 报 我 在 微官 。\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T07:12:04.578740Z",
     "start_time": "2024-12-07T07:12:04.568766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 示例文本\n",
    "text = \"请告严程尽，西归道路寒。欲陪鹰隼集，犹恋鹡鸰单。洛邑人全少，嵩高雪尚残。满台谁不故，报我在微官。\"\n",
    "\n",
    "# 文件路径\n",
    "VOCAB_FILE =\"./tang/vocab.pkl\"\n",
    "\n",
    "# 检查词表文件是否存在\n",
    "if not os.path.exists(VOCAB_FILE):\n",
    "    # 使用 Jieba 分词\n",
    "    words = jieba.lcut(text)\n",
    "\n",
    "    # 统计词频\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # 构建词表\n",
    "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "    # 保存词表映射\n",
    "    with open(VOCAB_FILE, 'wb') as f:\n",
    "        pickle.dump((word_to_index, index_to_word), f)\n",
    "else:\n",
    "    # 加载词表映射\n",
    "    with open(VOCAB_FILE, 'rb') as f:\n",
    "        word_to_index, index_to_word = pickle.load(f)\n",
    "\n",
    "# 定义编码函数\n",
    "def encode(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return [word_to_index[word] for word in words if word in word_to_index]\n",
    "\n",
    "# 定义解码函数\n",
    "def decode(indices):\n",
    "    return ' '.join(index_to_word[idx] for idx in indices if idx in index_to_word)\n",
    "\n",
    "# 示例编码和解码\n",
    "encoded_text = encode(text)\n",
    "decoded_text = decode(encoded_text)\n",
    "\n",
    "print(\"原始文本:\", text)\n",
    "print(\"编码结果:\", encoded_text)\n",
    "print(\"解码结果:\", decoded_text)\n"
   ],
   "id": "99abb00f66ad12b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本: 请告严程尽，西归道路寒。欲陪鹰隼集，犹恋鹡鸰单。洛邑人全少，嵩高雪尚残。满台谁不故，报我在微官。\n",
      "编码结果: [2, 3, 4, 0, 5, 6, 7, 1, 8, 9, 10, 11, 0, 12, 13, 14, 15, 1, 16, 17, 18, 0, 19, 20, 1, 21, 22, 23, 24, 0, 25, 26, 27, 28, 1]\n",
      "解码结果: 请告 严程 尽 ， 西归 道路 寒 。 欲 陪 鹰隼 集 ， 犹恋 鹡 鸰 单 。 洛邑 人 全少 ， 嵩高雪尚 残 。 满台 谁 不 故 ， 报 我 在 微官 。\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T07:19:40.952324Z",
     "start_time": "2024-12-07T07:19:40.945116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from opencc import OpenCC\n",
    "\n",
    "cc = OpenCC('t2s')\n",
    "text = [\"迨成祖，遷燕京，十七世，至崇禎。\",\n",
    "\t\t\"權閹肆，寇如林，至李闖，神器焚。\",\n",
    "\t\t\"清太祖，膺景命，靖四方，克大定。\",\n",
    "\t\t\"廿一史，全在茲，載治亂，知興衰。\",\n",
    "\t\t\"讀史者，考實錄，通古今，若親目。\",\n",
    "\t\t\"口而誦，心而惟，朝於斯，夕於斯。\",\n",
    "\t\t\"昔仲尼，師項橐，古聖賢，尚勤學。\",\n",
    "\t\t\"趙中令，讀魯論，彼既仕，學且勤。\",\n",
    "\t\t\"披蒲編，削竹簡，彼無書，且知勉。\",\n",
    "\t\t\"頭懸梁，錐刺股，彼不教，自勤苦。\",\n",
    "\t\t\"如囊螢，如映雪，家雖貧，學不輟。\",\n",
    "\t\t\"如負薪，如掛角，身雖勞，猶苦卓。\",\n",
    "\t\t\"蘇老泉，二十七，始發奮，讀書籍。\",\n",
    "\t\t\"彼既老，猶悔遲，爾小生，宜早思。\",\n",
    "\t\t\"若梁灝，八十二，對大廷，魁多士。\",\n",
    "\t\t\"彼既成，眾稱異，爾小生，宜立誌。\",\n",
    "\t\t\"瑩八歲，能詠詩，泌七歲，能賦碁。\",\n",
    "\t\t\"彼穎悟，人稱奇，爾幼學，當效之。\",\n",
    "\t\t\"蔡文姬，能辨琴，謝道韞，能詠吟。\",\n",
    "\t\t\"彼女子，且聰敏，爾男子，當自警。\",\n",
    "\t\t\"唐劉晏，方七歲，舉神童，作正字。\",\n",
    "\t\t\"彼雖幼，身已仕，爾幼學，勉而致。\",\n",
    "\t\t\"有為者，亦若是。\",\n",
    "\t\t\"犬守夜，雞司晨，茍不學，曷為人？\",\n",
    "\t\t\"蠶吐絲，蜂釀蜜，人不學，不如物。\",\n",
    "\t\t\"幼而學，壯而行，上致君，下澤民。\",\n",
    "\t\t\"揚名聲，顯父母，光於前，裕於後。\",\n",
    "\t\t\"人遺子，金滿籯，我教子，惟一經。\",\n",
    "\t\t\"勤有功，戲無益，戒之哉，宜勉力。\"]\n",
    "for sec in text:\n",
    "\tprint(cc.convert(sec))\n"
   ],
   "id": "19b4be63d2708c81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迨成祖，迁燕京，十七世，至崇祯。\n",
      "权阉肆，寇如林，至李闯，神器焚。\n",
      "清太祖，膺景命，靖四方，克大定。\n",
      "廿一史，全在兹，载治乱，知兴衰。\n",
      "读史者，考实录，通古今，若亲目。\n",
      "口而诵，心而惟，朝于斯，夕于斯。\n",
      "昔仲尼，师项橐，古圣贤，尚勤学。\n",
      "赵中令，读鲁论，彼既仕，学且勤。\n",
      "披蒲编，削竹简，彼无书，且知勉。\n",
      "头悬梁，锥刺股，彼不教，自勤苦。\n",
      "如囊萤，如映雪，家虽贫，学不辍。\n",
      "如负薪，如挂角，身虽劳，犹苦卓。\n",
      "苏老泉，二十七，始发奋，读书籍。\n",
      "彼既老，犹悔迟，尔小生，宜早思。\n",
      "若梁灏，八十二，对大廷，魁多士。\n",
      "彼既成，众称异，尔小生，宜立志。\n",
      "莹八岁，能咏诗，泌七岁，能赋碁。\n",
      "彼颖悟，人称奇，尔幼学，当效之。\n",
      "蔡文姬，能辨琴，谢道韫，能咏吟。\n",
      "彼女子，且聪敏，尔男子，当自警。\n",
      "唐刘晏，方七岁，举神童，作正字。\n",
      "彼虽幼，身已仕，尔幼学，勉而致。\n",
      "有为者，亦若是。\n",
      "犬守夜，鸡司晨，茍不学，曷为人？\n",
      "蚕吐丝，蜂酿蜜，人不学，不如物。\n",
      "幼而学，壮而行，上致君，下泽民。\n",
      "扬名声，显父母，光于前，裕于后。\n",
      "人遗子，金满籯，我教子，惟一经。\n",
      "勤有功，戏无益，戒之哉，宜勉力。\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:14:19.612004Z",
     "start_time": "2024-12-07T15:14:01.764891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from opencc import OpenCC\n",
    "\n",
    "def json2txt(path, save_path, key='paragraphs'):\n",
    "\t# 读取 JSON 文件\n",
    "\twith open(path, 'r', encoding='utf-8') as file:\n",
    "\t\tdata_list = json.load(file)\n",
    "\n",
    "\t# 提取所有 paragraphs 字段\n",
    "\tall_paragraphs = []\n",
    "\n",
    "\tfor item in data_list:\n",
    "\n",
    "\t\tparagraphs = item.get(key, [])\n",
    "\t\tpoem = str()\n",
    "\t\tfor paragraph in paragraphs:\n",
    "\t\t\t# 删除破折号及其后面的内容\n",
    "\t\t\tif '——' in paragraph:\n",
    "\t\t\t\tparagraph = paragraph.split('——')[0].strip()\n",
    "\t\t\t\tparagraph = re.sub(r'[a-zA-Z0-9]', '', paragraph)\n",
    "\t\t\tpoem+=paragraph\n",
    "\t\tall_paragraphs.append(poem)\n",
    "\n",
    "\t# 保存所有 paragraphs\n",
    "\twith open(save_path, 'a', encoding='utf-8') as output_file:\n",
    "\t\tfor paragraph in all_paragraphs:\n",
    "\t\t\toutput_file.write(cc.convert(paragraph)+'<|endoftext|>'+\"\\n\")\n",
    "\n",
    "def process_folder(folder_path, save_path, key='paragraphs'):\n",
    "    # 遍历文件夹中的所有文件\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            json2txt(file_path, save_path, key)\n",
    "\n",
    "cc = OpenCC('t2s')\n",
    "# 使用示例\n",
    "folder_path = r\"E:\\dataset\\chinese-poetry-master\\御定全唐詩\\json\"\n",
    "folder_path2 = r\"E:\\dataset\\chinese-poetry-master\\曹操诗集\"\n",
    "folder_path3 = r\"E:\\dataset\\chinese-poetry-master\\楚辞\"\n",
    "\n",
    "# process_folder(r\"E:\\dataset\\chinese-poetry-master\\纳兰性德\", \"./tang/nanlan.txt\", key='para')\n",
    "\n",
    "process_folder(r\"E:\\dataset\\chinese-poetry-master\\全唐诗\", r\"./tang/quantang.txt\")\n",
    "folder_path6 = r\"E:\\dataset\\chinese-poetry-master\\五代诗词\\huajianji\"\n",
    "\n",
    "# process_folder(folder_path6, \"./tang/huajianji.txt\")\n",
    "# process_folder(r\"E:\\dataset\\chinese-poetry-master\\五代诗词\\nantang\", r\"./tang/nantang.txt\")"
   ],
   "id": "4274ca26e6813814",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T08:34:32.340475Z",
     "start_time": "2024-12-07T08:34:32.065019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folder = \"./tang\"\n",
    "import os\n",
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "def read_files_in_folder(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "def build_vocab(texts, special_token='$$'):\n",
    "    # 将所有文本合并成一个字符串\n",
    "    combined_text = ''.join(texts)\n",
    "\n",
    "    # 使用 jeiba 分词\n",
    "    words = list(jieba.cut(combined_text))\n",
    "\n",
    "    # 处理特殊符号\n",
    "    special_tokens = [special_token]\n",
    "    words.extend(special_tokens * combined_text.count(special_token))\n",
    "\n",
    "    # 统计词频\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # 构建词表\n",
    "    vocab = list(word_counts.keys())\n",
    "    return vocab\n",
    "\n",
    "def save_vocab(vocab, save_path):\n",
    "    with open(save_path, 'w', encoding='utf-8') as file:\n",
    "        for word in vocab:\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "# 使用示例\n",
    "folder_path = \"./tang\"\n",
    "special_token = '<|endoftext|>'\n",
    "save_path = \"./tang/vocab.txt\"\n",
    "\n",
    "# 读取文件夹中的所有 .txt 文件\n",
    "texts = read_files_in_folder(folder_path)\n",
    "\n",
    "# 构建词表\n",
    "vocab = build_vocab(texts, special_token)\n",
    "\n",
    "# 保存词表\n",
    "save_vocab(vocab, save_path)\n"
   ],
   "id": "e6d5e7941cac7554",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jeiba'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./tang\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjeiba\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Counter\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_files_in_folder\u001B[39m(folder_path):\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'jeiba'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T08:50:53.554061Z",
     "start_time": "2024-12-07T08:50:50.857707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "\n",
    "# 加载 .npz 文件\n",
    "loaded_data = np.load('./tang/word_index_mapping.npz')\n",
    "\n",
    "# 提取数组\n",
    "words = loaded_data['words']\n",
    "indices = loaded_data['indices']\n",
    "\n",
    "# 重新构建字典\n",
    "word_to_index = {word: index for word, index in zip(words, indices)}\n",
    "index_to_word = {index: word for index, word in zip(indices, words)}\n",
    "\n",
    "# 打印验证\n",
    "print(\"word_to_index:\", word_to_index)\n",
    "print(\"index_to_word:\", index_to_word)\n",
    "\n",
    "# 定义编码函数\n",
    "def encode_text(text, word_to_index):\n",
    "    tokens = list(jieba.cut(text))\n",
    "    indices = [word_to_index[token] for token in tokens if token in word_to_index]\n",
    "    return indices\n",
    "\n",
    "# 定义解码函数\n",
    "def decode_indices(indices, index_to_word):\n",
    "    tokens = [index_to_word[index] for index in indices if index in index_to_word]\n",
    "    decoded_text = ''.join(tokens)\n",
    "    return decoded_text\n",
    "\n",
    "# 示例输入文本\n",
    "input_text = \"我喜欢编程\"\n",
    "\n",
    "# 编码\n",
    "encoded_indices = encode_text(input_text, word_to_index)\n",
    "print(\"Encoded Indices:\", encoded_indices)\n",
    "\n",
    "# 解码\n",
    "decoded_text = decode_indices(encoded_indices, index_to_word)\n",
    "print(\"Decoded Text:\", decoded_text)\n"
   ],
   "id": "5390ef65bbf14435",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\HP\\AppData\\Local\\Temp\\jieba.cache\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Loading model cost 0.726 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Indices: [6, 17484]\n",
      "Decoded Text: 我喜欢\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 直接加载词表进行分词和encoder",
   "id": "b98b3769299deb91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T13:27:03.814879Z",
     "start_time": "2024-12-07T13:27:02.728614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_vocab(vocab_file):\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "        vocab = [line.strip() for line in f.readlines()]\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    index_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "# 加载词汇表\n",
    "vocab_file = './tang/vocab.txt'\n",
    "word_to_index, index_to_word = load_vocab(vocab_file)\n",
    "\n"
   ],
   "id": "b3998aa44f36524e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T13:28:01.327803Z",
     "start_time": "2024-12-07T13:27:47.133347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_with_vocab(text, vocab):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        max_len = min(len(text) - i, max(len(word) for word in vocab))\n",
    "        found = False\n",
    "        for length in range(max_len, 0, -1):\n",
    "            candidate = text[i:i + length]\n",
    "            if candidate in vocab:\n",
    "                tokens.append(candidate)\n",
    "                i += length\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            tokens.append(text[i])\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "def encode_text_with_vocab(text, word_to_index, vocab):\n",
    "    tokens = tokenize_with_vocab(text, vocab)\n",
    "    print(\"Tokens from vocab:\", tokens)  # 打印分词结果\n",
    "\n",
    "    encoded_indices = []\n",
    "    for token in tokens:\n",
    "        if token in word_to_index:\n",
    "            encoded_indices.append(word_to_index[token])\n",
    "        else:\n",
    "            # 处理未知词，可以选择跳过或使用特殊标记\n",
    "            unknown_token = '[UNK]'\n",
    "            if unknown_token in word_to_index:\n",
    "                encoded_indices.append(word_to_index[unknown_token])\n",
    "            else:\n",
    "                encoded_indices.append(len(word_to_index))  # 假设 [UNK] 是最后一个词\n",
    "            print(f\"Unknown token '{token}' replaced with '{unknown_token}'\")  # 打印未知词替换信息\n",
    "    return encoded_indices\n",
    "\n",
    "def decode_indices(indices, index_to_word):\n",
    "    tokens = [index_to_word.get(index, '[UNK]') for index in indices]  # 处理未知索引\n",
    "    decoded_text = ''.join(tokens)\n",
    "    return decoded_text\n",
    "\n",
    "# 示例输入文本\n",
    "# input_text = \"<|endoftext|>舳舻衔尾日无虚，更凿都城引漕渠。何事馁来贪雀谷，不知留得几年储。竹影桐阴满旧山，凤凰多载不飞还。登台只有吹箫者，争得和鸣堕世间。<|endoftext|>\"\n",
    "input_text = \"明主不弃士，我自志山林。爵服岂无华，才疏力难任。鸟向深山栖，鱼由深渊沉。吾亦爱吾庐，高歌复微吟。<|endoftext|>深岩有老翁，庞眉须鬓雪。夜半呼我名，授我微妙诀。字画古籀样，体势讹复缺。双眸忽炯炯，须臾竟披阅。至今得其传，心会口难说。\"\n",
    "\n",
    "# 获取词汇表列表\n",
    "vocab = list(word_to_index.keys())\n",
    "\n",
    "# 编码\n",
    "encoded_indices = encode_text_with_vocab(input_text, word_to_index, vocab)\n",
    "print(\"Encoded Indices:\", encoded_indices)\n",
    "\n",
    "# 解码\n",
    "# decoded_text = decode_indices(encoded_indices, index_to_word)\n",
    "# print(\"Decoded Text:\", decoded_text)\n"
   ],
   "id": "fdf4de985921463d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens from vocab: ['明主', '不弃士', '，', '我', '自志', '山林', '。', '爵服', '岂', '无华', '，', '才', '疏力', '难任', '。', '鸟向', '深山', '栖', '，', '鱼', '由', '深渊', '沉', '。', '吾', '亦', '爱吾庐', '，', '高歌', '复微吟', '。', '<|endoftext|>', '深岩', '有', '老翁', '，', '庞眉须', '鬓雪', '。', '夜半', '呼', '我名', '，', '授', '我', '微妙', '诀', '。', '字画', '古', '籀', '样', '，', '体势', '讹', '复缺', '。', '双眸', '忽', '炯炯', '，', '须臾', '竟', '披阅', '。', '至今', '得', '其传', '，', '心会口', '难说', '。']\n",
      "Encoded Indices: [10764, 10765, 1, 99, 10766, 10767, 4, 10768, 61, 10769, 1, 1410, 10770, 5067, 4, 10771, 8741, 472, 1, 4698, 1040, 10772, 5171, 4, 194, 769, 10773, 1, 6057, 10774, 4, 1273723, 10775, 24, 10776, 1, 10777, 30432, 4, 2809, 10778, 269071, 1, 10779, 99, 10780, 10781, 4, 10782, 8927, 10783, 10784, 1, 10785, 10786, 9487, 4, 10787, 3901, 10788, 1, 10485, 1246, 10789, 4, 95, 167, 10790, 1, 10791, 3103, 4]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 使用jieba加载本地词表，进行分词",
   "id": "9e60067bdb9a6516"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T09:08:59.764549Z",
     "start_time": "2024-12-07T09:08:34.560644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jieba\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "        vocab = [line.strip() for line in f.readlines()]\n",
    "    return vocab\n",
    "\n",
    "# 加载词汇表\n",
    "vocab_file = './tang/vocab.txt'\n",
    "vocab = load_vocab(vocab_file)\n",
    "\n",
    "# 将词汇表添加到 jieba 的用户词典\n",
    "for word in vocab:\n",
    "    jieba.add_word(word)\n",
    "def encode_text_with_vocab(text, word_to_index):\n",
    "    tokens = list(jieba.cut(text))\n",
    "    print(\"Tokens from jieba:\", tokens)  # 打印分词结果\n",
    "\n",
    "    encoded_indices = []\n",
    "    for token in tokens:\n",
    "        if token in word_to_index:\n",
    "            encoded_indices.append(word_to_index[token])\n",
    "        else:\n",
    "            # 处理未知词，可以选择跳过或使用特殊标记\n",
    "            unknown_token = '[UNK]'\n",
    "            if unknown_token in word_to_index:\n",
    "                encoded_indices.append(word_to_index[unknown_token])\n",
    "            else:\n",
    "                encoded_indices.append(len(word_to_index))  # 假设 [UNK] 是最后一个词\n",
    "            print(f\"Unknown token '{token}' replaced with '{unknown_token}'\")  # 打印未知词替换信息\n",
    "    return encoded_indices\n",
    "\n",
    "def decode_indices(indices, index_to_word):\n",
    "    tokens = [index_to_word.get(index, '[UNK]') for index in indices]  # 处理未知索引\n",
    "    decoded_text = ''.join(tokens)\n",
    "    return decoded_text\n",
    "\n",
    "# 示例输入文本\n",
    "input_text = \"<|endoftext|>舳舻衔尾日无虚，更凿都城引漕渠。何事馁来贪雀谷，不知留得几年储。竹影桐阴满旧山，凤凰多载不飞还。登台只有吹箫者，争得和鸣堕世间。<|endoftext|>\"\n",
    "\n",
    "# 加载词汇表\n",
    "vocab_file = './tang/vocab.txt'\n",
    "vocab = load_vocab(vocab_file)\n",
    "\n",
    "# 将词汇表添加到 jieba 的用户词典\n",
    "for word in vocab:\n",
    "    jieba.add_word(word)\n",
    "\n",
    "# 加载词汇表并构建映射\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "# 编码\n",
    "encoded_indices = encode_text_with_vocab(input_text, word_to_index)\n",
    "print(\"Encoded Indices:\", encoded_indices)\n",
    "\n",
    "# 解码\n",
    "decoded_text = decode_indices(encoded_indices, index_to_word)\n",
    "print(\"Decoded Text:\", decoded_text)\n"
   ],
   "id": "3c7f61c66c35fbf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens from jieba: ['<', '|', 'endoftext', '|', '>', '舳舻', '衔尾', '日无虚', '，', '更凿', '都城', '引', '漕渠', '。', '何事', '馁', '来', '贪雀谷', '，', '不知', '留得', '几年', '储', '。', '竹影', '桐阴满', '旧山', '，', '凤凰', '多载', '不飞', '还', '。', '登台', '只有', '吹箫', '者', '，', '争得', '和', '鸣', '堕', '世间', '。', '<', '|', 'endoftext', '|', '>']\n",
      "Encoded Indices: [67, 68, 69, 68, 70, 10670, 10671, 10672, 1, 329805, 10674, 4590, 10675, 4, 1702, 10676, 110, 10677, 1, 1062, 5881, 2412, 255, 4, 7153, 10678, 10679, 1, 1316, 10680, 10681, 363, 4, 10682, 10617, 4286, 46, 1, 10527, 2332, 2982, 829, 4296, 4, 67, 68, 69, 68, 70]\n",
      "Decoded Text: <|endoftext|>舳舻衔尾日无虚，更凿都城引漕渠。何事馁来贪雀谷，不知留得几年储。竹影桐阴满旧山，凤凰多载不飞还。登台只有吹箫者，争得和鸣堕世间。<|endoftext|>\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
